\documentclass[12pt]{article}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{mathrsfs}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{hyperref}

\titleformat{\section}{\normalfont\Large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{0.5em}{}
\setlist[itemize]{noitemsep, topsep=0pt}

\begin{document}

\begin{center}
    {\LARGE \textbf{Линейная алгебра. Модуль: Евклидовы пространства. Тензоры}}\\
    \vspace{0.3em}
    Башков Иван, Кунгурцев Максим, Макашов Глеб, Малиновский Степан\\
    \vspace{0.3em}
    11 июня 2025 г.
\end{center}

\section{Билинейные функции и их матрицы. Преобразование матрицы билинейной функции при замене базиса. Ранг и ядра билинейной функции. Связь между билинейной функцией и линейным отображением в сопряжённое пространство.}

\subsection{Билинейные функции и их матрицы}

\textbf{Определение.} \textit{Билинейной функцией (формой)} на векторном пространстве $V$ называется отображение 
\[
\beta: V \times V \to \mathbb{F},
\]
которое линейно по каждому аргументу.

Это означает, что:
\begin{itemize}
    \item $\beta(x_1 + x_2, y) = \beta(x_1, y) + \beta(x_2, y)$,
    \item $\beta(\lambda x, y) = \lambda \beta(x, y)$,
    \item $\beta(x, y_1 + y_2) = \beta(x, y_1) + \beta(x, y_2)$,
    \item $\beta(x, \lambda y) = \lambda \beta(x, y)$,
\end{itemize}
для всех $x, x_1, x_2, y, y_1, y_2 \in V$, $\lambda \in \mathbb{F}$.

\vspace{0.5em}
\textbf{Матрица билинейной формы.} Пусть $e_1, \dots, e_n$ — базис в $V$, тогда билинейная форма $\beta$ задаётся значениями $\beta(e_i, e_j)$, которые образуют матрицу $B = (\beta(e_i, e_j))$.

Если $x = \sum x_i e_i$, $y = \sum y_j e_j$, то:
\[
\beta(x, y) = X^\top B Y, \quad \text{где } X = (x_1, \dots, x_n)^\top,\ Y = (y_1, \dots, y_n).
\]

\subsection{Преобразование матрицы билинейной формы при замене базиса}

Пусть $C$ — матрица перехода от базиса $e$ к новому базису $\tilde{e}$. Тогда матрица билинейной формы в новом базисе $\tilde{B}$ связана с исходной по формуле:
\[
\tilde{B} = C^\top B C.
\]

\textbf{Доказательство:}
Пусть $C=(e \rightsquigarrow \tilde{e})$, где $e$ – старый базис, $\tilde{e}$ – новый базис. Тогда\\
$X = (e \rightsquigarrow \tilde{e}) \tilde{X} = C\tilde{X}$, $Y = (e \rightsquigarrow \tilde{e}) \tilde{Y} = C\tilde{Y}$\\
$\beta(x,y)=(C \tilde{X})^\top B(C\tilde{Y})=\tilde{X}^\top(C^\top BC)\tilde{Y}=\tilde{X}^\top B\tilde{Y} \implies \tilde{B} = C^\top BC, \ C \in GL_n(F) \ \square$

\textbf{Следствие.} Ранг билинейной формы не зависит от выбора базиса, так как $C$ невырожденная.

\subsection{Ранг и ядра билинейной функции}

\textbf{Определение.} \textit{Рангом билинейной формы} $\beta$ называется ранг её матрицы:
\[
\mathrm{rk}\, \beta = \mathrm{rk}\, B.
\]

\textbf{Правое ядро:}
\[
R_\beta =\mathrm{Ker} \beta = \{ y \in V \mid \beta(x, y) = 0 \ \ \forall x \in V \}.
\]

\textbf{Левое ядро:}
\[
L_ \beta = \{ x \in V \mid \beta(x, y) = 0 \ \ \forall y \in V \}.
\]

\textbf{Определение.} Форма называется \textit{невырожденной}, если $\mathrm{Ker} \beta = \{0\}$.

\textbf{Лемма.} $\beta{} \text{ невырождена } \Longleftrightarrow rk\beta=n.$

\subsection{Связь с линейным отображением в сопряжённое пространство}

\textbf{Теорема.} Билинейные формы на $V$ канонически изоморфны линейным отображениям из $V$ в его сопряжённое пространство $V^*$.

\textbf{Доказательство:}

1. Для каждой билинейной формы $\beta \in BL(V)$ определим отображение $\varphi(\beta): V \to V^*$ следующим образом:
     $$
     \varphi(\beta)(v) = \beta(v, -),
     $$
     где $\beta(v, -)$ — линейный функционал на $V$, определенный как:
     $$
     \beta(v, -)(w) = \beta(v, w), \quad \forall w \in V.
     $$

2. Линейность $\varphi$. Для любых $\beta_1, \beta_2 \in BL(V)$ и $\alpha \in F$:
       $$
       \varphi(\beta_1 + \alpha \beta_2)(v) = (\beta_1 + \alpha \beta_2)(v, -) = \beta_1(v, -) + \alpha \beta_2(v, -).
       $$
       Это означает:
       $$
       \varphi(\beta_1 + \alpha \beta_2) = \varphi(\beta_1) + \alpha \varphi(\beta_2).
       $$
       Таким образом, $\varphi$ — линейное отображение.

3. Ядро $\varphi$ состоит из всех билинейных форм $\beta$, для которых $\varphi(\beta) = 0$, то есть:
     $$
     \mathrm{Ker} \varphi = \{\beta \in BL(V) \mid \varphi(\beta) = 0\}.
     $$
     Если $\varphi(\beta) = 0$, то для любого $v \in V$:
     $$
     \beta(v, -) = 0,
     $$
     что означает, что $\beta(v, w) = 0$ для всех $v, w \in V$. Следовательно, $\beta = 0$. Таким образом:
     $$
     \mathrm{Ker} \varphi = \{0\}.
     $$

4. Образ $\varphi$ состоит из всех линейных отображений $V \to V^*$, которые можно получить из билинейных форм. Для любого отображения существует $\beta$, что $\varphi(\beta)$ – это именно такое линейное отображение. Таким образом:
     $$
     \mathrm{Im} \varphi \leq \mathrm{Hom}(V, V^*), \ \dim \mathrm{Im}\varphi=\dim \mathrm{Hom}(V,V^*)=\dim BL(V) \implies \mathrm{Im}\varphi = \mathrm{Hom}(V,V^*)
     $$

5. Поскольку $\mathrm{Ker} \varphi = \{0\}$ и $\mathrm{Im} \, \varphi = \mathrm{Hom}(V, V^*)$, отображение $\varphi$ является \textbf{изоморфизмом} между $BL(V)$ и $\mathrm{Hom}(V, V^*)$. $\ \ \square$



\section{Симметрические и кососимметрические билинейные функции, их матрицы. Ортогональное дополнение к подпространству относительно билинейной функции, его свойства.}

\subsection{Симметрические и кососимметрические билинейные функции, их матрицы}

\textbf{Определение.} Билинейная форма $\beta$ называется:
\begin{itemize}
    \item \textit{симметрической} $(BL^+(V))$, если $\beta(x, y) = \beta(y, x) \ \forall x, y \in V$;
    \item \textit{кососимметрической} $(BL^-(V))$, если $\beta(x, y) = -\beta(y, x) \ \forall x, y \in V$;
    \item \textit{антисимметрической (симплектической)}, если $\beta(x,x)=0 \ \forall x \in V$.
\end{itemize}

\textbf{Лемма.} Если $\beta$ антисимметрическая, то $\beta \in BL^-(V)$.

\textbf{Доказательство.}
$0 = \beta(x + y, x + y)
  = \beta(x, x) + \beta(x, y) + \beta(y, x) + \beta(y, y) = \beta(x, y) + \beta(y, x)
  \implies \beta(x, y) = -\beta(y, x).$ \\
В доказательстве использовалось $\beta(x,x)=0$. Это верно при $char \mathbb{F} \ne 2$: $\beta(x,x)=-\beta(x,x) \implies 2\beta(x,x)=0$.


\textbf{Лемма.} $BL(V) = BL^+(V) \oplus BL^-(V)$.

\textbf{Доказательство:}

\textbf{I.} $BL^+(V) \cap BL^-(V) = \{0\}$

Пусть $\beta \in BL^+(V) \cap BL^-(V)$. Тогда:
\[
\beta(x, y) = \beta(y, x) = -\beta(x, y) \implies 2\beta(x, y) = 0 \implies \beta = 0.
\]

\textbf{II.} Для любой билинейной формы $\beta(x, y)$ можно записать:
\[
\beta(x, y) = \frac{\beta(x, y) + \beta(y, x)}{2} + \frac{\beta(x, y) - \beta(y, x)}{2}.
\]
Здесь:
\[
\frac{\beta(x, y) + \beta(y, x)}{2} \in BL^+(V), \quad \frac{\beta(x, y) - \beta(y, x)}{2} \in BL^-(V). \ \ \square
\]

\textbf{Матрица билинейной формы.}
Пусть $B = (\beta(e_i, e_j))$ — матрица формы в базисе $e_1, \dots, e_n$.
\begin{itemize}
    \item $\beta$ симметрическая $\iff B = B^\top$;
    \item $\beta$ кососимметрическая $\iff B = -B^\top$.
\end{itemize}

\textbf{Следствие.} Ранг кососимметрической формы — чётное число.

\subsection{Ортогональное дополнение относительно билинейной формы}

Пусть $\beta$ – симметрическая/кососимметрическая билинейная форма. Векторы $x$ и $y$ \textbf{ортогональны} относительно $\beta$, если $\beta(x,y)=0$.

\textbf{Определение.} Пусть $U \leq V$. Ортогональным дополнением $U^\perp$ относительно формы $\beta$ называется:
\[
U^\perp = \{ y \in V \mid \beta(x, y) = 0 \ \forall x \in U \}.
\]

\textbf{Лемма.} Если $\beta$ невырожденная, то:
\begin{itemize}
    \textbf{I.} $\dim U^\perp = \dim V - \dim U$;\\
    \textbf{II.} $(U^\perp)^\perp = U$.
\end{itemize}

\textbf{Доказательство:}

\textbf{I.} Пусть $ e_1, \dots, e_k, e_{k+1}, \dots, e_n $ — базис пространства $ V $ и $ e_1, \dots, e_k $ — базис подпространства $ U $.

Рассмотрим ортогональное дополнение $ U^\perp $:
\[
U^\perp = \{ y \in V \mid \beta(e_i, y) = 0 \quad \forall i = 1, \dots, k \}.
\]

Пусть $ y = e_1 y_1 + \dots + e_n y_n $. Тогда система уравнений для $ y \in U^\perp $ имеет вид:
\[
\begin{cases}
b_{11} y_1 + \dots + b_{1n} y_n = 0 \\
b_{21} y_1 + \dots + b_{2n} y_n = 0 \\
\vdots \\
b_{k1} y_1 + \dots + b_{kn} y_n = 0
\end{cases}
\]

Это система линейных алгебраических уравнений (СЛАУ). Ранг матрицы системы равен рангу матрицы коэффициентов:
\[
\mathrm{rk} \begin{pmatrix}
b_{11} & \dots & b_{1n} \\
b_{21} & \dots & b_{2n} \\
\vdots & \ddots & \vdots \\
b_{k1} & \dots & b_{kn}
\end{pmatrix}.
\]

Ранг матрицы коэффициентов, в свою очередь, равен $\dim U$, так как $\beta$ невырождена $\implies$ любая подсистема строк в $B$ линейно независима.

Таким образом, размерность $ U^\perp $ вычисляется как:
\[
\dim U^\perp = n - \mathrm{rk} \begin{pmatrix}
b_{11} & \dots & b_{1n} \\
b_{21} & \dots & b_{2n} \\
\vdots & \ddots & \vdots \\
b_{k1} & \dots & b_{kn}
\end{pmatrix} = n - k = \dim V - \dim U.
\]

\textbf{II.} 1. Проверим включение $ U \subseteq (U^\perp)^\perp $:
   - Возьмём произвольный вектор $ u \in U $. По определению $ U^\perp $, для любого $ y \in U^\perp $ выполнено $ \beta(u, y) = 0 $.
   Следовательно, $ u \in (U^\perp)^\perp $, так как $ u $ ортогонален всем векторам из $ U^\perp $.

2. Вычислим размерность $ (U^\perp)^\perp $:
   по первой части, $ \dim U^\perp = n - k $.
   Аналогично, $ \dim (U^\perp)^\perp = \dim V - \dim U^\perp = n - (n - k) = k $.
   Так как $ \dim U = k $, получаем $ \dim (U^\perp)^\perp = \dim U $.

3. Поскольку $ U \subseteq (U^\perp)^\perp $ и размерности совпадают, имеем $ U = (U^\perp)^\perp. \ \ \square$


\textbf{Определение.} Подпространство $U$ называется \textit{невырожденным}, если ограничение формы $\beta|_U$ — невырожденное.

\textbf{Лемма.} $U$ невырожденное $\iff V = U \oplus U^\perp$.

\textbf{Доказательство.}

$\dim U = k, \ \dim U^\perp \geq n-k$

$\Longrightarrow$

$\dim (U+U^\perp)=\dim U + \dim U^\perp - \dim (U \cap U^\perp)$

$U \cap U^\perp = \set {0}, \dim(U+U^\perp)=\dim U+\dim U^\perp \geq k + (n-k) \geq n = \dim V$

$U+U^\perp \leq V \implies V = U \oplus U^\perp$

\Longleftarrow

$V = U \oplus U^\perp \implies V = U + U^\perp$

$n = \dim (U+U^\perp) = \dim U + \dim U^\perp \implies \dim(U \cap U^\perp)=0 \implies U \cap U^\perp = \set{0} \ \ \square$

\textbf{Определение.} Базис $e_1, \dots, e_n$ называется \textit{ортогональным}, если $\beta(e_i, e_j) = 0$ при $i \ne j$.

\textbf{Теорема.} Для любой симметрической билинейной формы на конечномерном пространстве существует ортогональный базис.

\textbf{Доказательство.} Индукция по $n = \dim V$:
\begin{itemize}
    \item База: $n = 1$ — очев.
    \item Шаг: $n = k$. Пусть $e_k \in V$ так, что $\beta(e_k, e_k) \ne 0$. Тогда $\beta$ невырождена на $U = \langle e_k \rangle$, и по невырожденности $\beta|_U$ имеем разложение $V = U \oplus U^\perp$, при этом $\dim U^\perp=k-1$. По предположению индукции в $U^\perp$ существует ортогональный базис. Добавим к нему $e_k$ и получим базис $V$.$\ \ \square$
\end{itemize}

\section{Квадратичные функции, поляризация. Канонический и нормальный виды симметрической билинейной и квадратичной функций.}

\subsection{Квадратичные функции, поляризация}

\textbf{Определение.} Пусть $\beta \in BL^+(V).$ \textit{Квадратичная форма} – $q(x)=\beta(x,x)$:

$$
q(x) =\sum_{1 \leq i \leq j \leq n}{x_i b_{ij}x_j}
$$

\textbf{Поляризация квадратичной формы} — восстановление билинейной формы:

$$
q(x + y) = \beta(x + y, x + y)
         = \beta(x, x) + 2\beta(x, y) + \beta(y, y)
$$

$$
\beta(x, y) = \frac{q(x + y) - q(x) - q(y)}{2}
$$

\subsection{Канонический и нормальный виды}

Для любой симметрической билинейной формы существует базис, в котором она имеет вид:
$$
\beta(x, y) = \sum_{i=1}^n a_i x_i y_i.
$$

Соответствующая квадратичная форма:
$$
q(x) = \sum_{i=1}^n a_i x_i^2.
$$

Нормальный вид — канонический вид, в котором все ненулевые коэффициенты равны $\pm 1$.

\section{Методы Лагранжа и Якоби приведения квадратичной функции к каноническому виду. Закон инерции.}

\section{Положительно/отрицательно определённые билинейные и квадратичные функции. Критерий Сильвестра.}

\subsection*{Билинейные формы}

\textbf{Определение:} Билинейная форма $B(\vec{x}, \vec{y})$ называется \textbf{положительно определённой}, если $\forall \vec{x} \in L_n$, $\vec{x} \neq \vec{0}: B(\vec{x}, \vec{x}) > 0$.

\textbf{Определение:} Билинейная форма $B(\vec{x}, \vec{y})$ называется \textbf{отрицательно определённой}, если $\forall \vec{x} \in L_n$, $\vec{x} \neq \vec{0}: B(\vec{x}, \vec{x}) < 0$.

\subsection*{Полуторалинейные формы}

\textbf{Определение:} Полуторалинейная форма $B(\vec{x}, \vec{y})$, заданная в $L_n(\mathbb{C})$, называется \textbf{положительно определённой}, если $\forall \vec{x} \in L_n$, $\vec{x} \neq \vec{0}: B(\vec{x}, \vec{x}) > 0$.



\textbf{Определение:} Полуторалинейная форма $B(\vec{x}, \vec{y})$, заданная в $L_n(\mathbb{C})$, называется \textbf{отрицательно определённой}, если $\forall \vec{x} \in L_n$, $\vec{x} \neq \vec{0}: B(\vec{x}, \vec{x}) < 0$.



Отметим, что $B(\vec{x}, \vec{x}) = \overline{B(\vec{x}, \vec{x})}$.



\subsection*{Квадратичные формы}

\textbf{Определение:} Квадратичная форма $Q(\vec{x})$ называется \textbf{положительно определённой}, если $\forall \vec{x} \neq \vec{0}: Q(\vec{x}) > 0$.



\textbf{Определение:} Квадратичная форма $Q(\vec{x})$ называется \textbf{отрицательно определённой}, если $\forall \vec{x} \neq \vec{0}: Q(\vec{x}) < 0$.



\subsection*{Доказательство критерия Сильвестра}


\subsubsection*{Необходимость}
Докажем, что если симметричная билинейная форма $\beta$ положительно определена, то все её угловые миноры положительны.

\textbf{Шаг 1: База индукции ($n = 1$)} \\
При $n = 1$ всё очевидно: матрица билинейной функции $\beta$ в любом базисе представляет собой одну клетку $A = [a_{1,1}]$, а в координатной записи получим
\begin{align*}
q(x) &= a_{1,1} x_1^2 > 0, \\
\Delta_1 &= a_{1,1} > 0.
\end{align*}

Поскольку функция $\beta$ положительно определена на всём пространстве $V$, её ограничение $\beta|_U$ на подпространство $U$ тоже положительно определено. Матрица ограничения $\beta|_U$ (назовём её $A|_U$) получается из матрицы $A$ вычеркиванием последней строки и последнего столбца:

\[
A =
\begin{bmatrix}
a_{1,1} & \cdots & a_{1,k} & a_{1,k+1} \\
\vdots & \ddots & \vdots & \vdots \\
a_{k,1} & \cdots & a_{k,k} & a_{k,k+1} \\
a_{k+1,1} & \cdots & a_{k+1,k} & a_{k+1,k+1}
\end{bmatrix}, \quad
A|_U =
\begin{bmatrix}
a_{1,1} & \cdots & a_{1,k} \\
\vdots & \ddots & \vdots \\
a_{k,1} & \cdots & a_{k,k}
\end{bmatrix}.
\]

Видно, что угловые миноры $\Delta_1, \dots, \Delta_k$ у матриц $A$ и $A|_U$ совпадают. Однако $\dim U = k$, а по предположению индукции для положительно определённой $\beta|_U$ все угловые миноры положительны:
\[
\Delta_1 = a_{1,1} > 0, \quad \dots, \quad \Delta_k = \det(A|_U) > 0.
\]

Осталось доказать, что минор $\Delta_{k+1} = \det A > 0$.

По теореме о диагональной матрице обязательно найдётся базис $\{f_1, \dots, f_{k+1}\}$ такой, что матрица $B$ билинейной функции $\beta$ в этом базисе диагональна:
\[
B =
\begin{bmatrix}
b_{1,1} & \cdots & 0 & 0 \\
\vdots & \ddots & \vdots & \vdots \\
0 & \cdots & b_{k,k} & 0 \\
0 & \cdots & 0 & b_{k+1,k+1}
\end{bmatrix}.
\]

Поскольку функция $\beta$ положительно определена, на диагонали матрицы $B$ стоят только положительные элементы:
\[
b_{i,i} = \beta(f_i, f_i) > 0, \quad 1 \leq i \leq k+1.
\]

Следовательно, определитель диагональной матрицы $B$ тоже положительный:
\[
\det B = b_{1,1} \cdot b_{2,2} \cdot \ldots \cdot b_{k+1,k+1} > 0.
\]

С другой стороны, мы знаем, что матрицы $A$ и $B$ связаны с матрицей перехода $T = T_{e \to f}$ по формуле
\[
B = T^T \cdot A \cdot T.
\]

Следовательно, определители этих матриц тоже связаны:
\begin{align*}
\det B &= \det(T^T \cdot A \cdot T) = \\
&= \det(T^T) \cdot \det A \cdot \det T = \\
&= (\det T)^2 \cdot \Delta_{k+1}.
\end{align*}

Здесь мы воспользовались свойствами определителей:

    
- Определитель произведения равен произведению определителей.
    
- При транспонировании матрицы её определитель не меняется.


Выше мы доказали, что $\det B > 0$. Кроме того, $\det T \neq 0$, поскольку матрица перехода не вырождена. Следовательно, $\Delta_{k+1} > 0$, что и требовалось доказать.


\subsubsection*{Достаточность}
Докажем, что если все угловые миноры $\Delta_1, \dots, \Delta_n$ матрицы $A$ билинейной функции $\beta$ в некотором базисе $\{e_1, \dots, e_n\}$ положительны, то функция $\beta$ положительно определена.

\textbf{Шаг 1: Формула Якоби} \\
Согласно формуле Якоби, в пространстве $V$ существует базис $\{f_1, \dots, f_n\}$ такой, что квадратичная функция $q$, ассоциированная с билинейной функцией $\beta$, принимает вид:
\[
q(x) = \frac{\Delta_1}{\Delta_0} x_1^2 + \frac{\Delta_2}{\Delta_1} x_2^2 + \ldots + \frac{\Delta_n}{\Delta_{n-1}} x_n^2,
\]
где $\Delta_0 = 1$.

\textbf{Шаг 2: Анализ коэффициентов} \\
Заметим, что коэффициенты $\frac{\Delta_i}{\Delta_{i-1}} > 0$ для всех $1 \leq i \leq n$. Следовательно, $q(x)$ представляет собой сумму $n$ неотрицательных слагаемых. Поскольку вектор $x \neq 0$, хотя бы одно из этих слагаемых точно будет положительным, поэтому $q(x) > 0$, что и требовалось доказать.

\subsubsection*{Относительно отрицательно определённых функций}
Аналогично критерию Сильвестра для положительно определённых функций, доказательство для отрицательно определённых функций проводится с той лишь разницей, что в подходящем базисе $\{f_1, \dots, f_n\}$ все элементы диагональной матрицы $B$ будут отрицательными:
\[
B =
\begin{bmatrix}
-|b_{1,1}| & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & -|b_{n,n}|
\end{bmatrix}, \quad b_{i,i} < 0.
\]

Следовательно, определитель диагональной матрицы $B$ будет положительным, если $n$ чётное, и отрицательным, если $n$ нечётное. Это приводит к чередованию знаков угловых миноров, начиная с $\Delta_1 < 0$:
\[
\Delta_1 < 0, \quad \Delta_2 > 0, \quad \Delta_3 < 0, \quad \dots
\]

\section{Евклидовы векторные пространства: определение и примеры. Неравенство Коши-Буняковского, неравенство треугольника, теорема Пифагора. Длина и угол между екторами, теорема косинусов.}

\subsection*{Определение}
\underline{Определение}: Линейное пространство $L_n$ называется \textbf{евклидовым} ($E_n$), если в нём определена симметричная, положительно определённая БФ $G(\vec{x}, \vec{y})$, называемая \textit{скалярным произведением} $(\vec{x}, \vec{y})$.

\subsection*{Определение}
(ещё одно определение для понятности) Чтобы дать определение евклидова пространства, в качестве основы проще всего использовать понятие \textit{скалярного произведения}. Евклидово векторное пространство определяется как \textbf{конечномерное векторное пространство над полем вещественных чисел}, на парах векторов которого задана вещественнозначная функция $(\cdot, \cdot)$, обладающая следующими тремя свойствами:


    
\textbf{Линейность}: для любых векторов $\vec{u}, \vec{v}, \vec{w}$ и для любых вещественных чисел $a, b$ справедливы соотношения  
    $$
    (a\vec{u} + b\vec{v}, \vec{w}) = a(\vec{u}, \vec{w}) + b(\vec{v}, \vec{w});
    $$
    
\textbf{Симметричность}: для любых векторов $\vec{u}, \vec{v}$ верно равенство  
    $$
    (\vec{u}, \vec{v}) = (\vec{v}, \vec{u});
    $$
    
\textbf{Положительная определённость}: $(\vec{u}, \vec{u}) \geq 0$ для любого $\vec{u}$, причём $(\vec{u}, \vec{u}) = 0 \Rightarrow \vec{u} = \vec{0}$.


\subsection*{Примеры}
\underline{Пример 1}: Аффинное пространство, ассоциированное с векторным пространством $V$ над полем $\mathbb{K}$, — это множество $A$ со свободным транзитивным действием аддитивной группы $V$. Если поле $\mathbb{K}$ не указано явно, подразумевается $\mathbb{K} = \mathbb{R}$.

\underline{Пример 2}: Координатное пространство $\mathbb{R}^n$, состоящее из наборов вещественных чисел $(x_1, x_2, \dots, x_n)$, где скалярное произведение определяется формулой:
$$
(\vec{x}, \vec{y}) = \sum_{i=1}^n x_i y_i = x_1 y_1 + x_2 y_2 + \dots + x_n y_n.
$$

\underline{Пример 3}: Пространство многочленов ограниченной степени $P_n$ — множество всех многочленов с вещественными коэффициентами степени не выше $n$:  
$$
p(t) = a_0 + a_1 t + a_2 t^2 + \dots + a_n t^n.
$$  
Скалярное произведение задаётся формулой:  
$$
(p, q) = \int_a^b p(t)q(t)\,dt,
$$  
где $[a, b]$ — фиксированный интервал ($[0, 1]$ или $[-1, 1]$).  

\textit{Комментарий}: Это евклидово пространство, где векторами являются функции. Проверка аксиом скалярного произведения (симметрия, билинейность, положительная определённость) — стандартное упражнение. Норма многочлена:  
$$
\|p\| = \sqrt{\int_a^b [p(t)]^2\,dt}.
$$


\subsection*{Неравенство Коши-Буняковского}
\underline{Формулировка}:  
Пусть дано линейное пространство $L$ со скалярным произведением $\langle x, y \rangle$. Пусть $\|x\| = \sqrt{\langle x, x \rangle}$ — норма, порождённая скалярным произведением, $\forall x \in L$. Тогда для любых $x, y \in L$ выполняется:
$$
|\langle x, y \rangle| \leq \|x\| \cdot \|y\|.
$$
Равенство достигается тогда и только тогда, когда векторы $x$ и $y$ линейно зависимы (\textit{коллинеарны}), или среди них есть нулевой вектор.

\subsection*{Доказательство (метод квадратичной формы)}
Рассмотрим конструкцию $(x + \lambda y, x + \lambda y)$ для любых $x, y \in E_n$ и $\lambda \in \mathbb{R}$. По свойствам скалярного произведения:
$$
(x + \lambda y, x + \lambda y) \geq 0.
$$
Раскроем скалярное произведение:
$$
(x, x) + \lambda (x, y) + \lambda (y, x) + \lambda^2 (y, y) \geq 0.
$$
Учитывая симметричность скалярного произведения $(x, y) = (y, x)$, получаем:
$$
\lambda^2 (y, y) + 2\lambda (x, y) + (x, x) \geq 0.
$$
Это квадратное выражение относительно $\lambda$ должно быть неотрицательным для всех $\lambda \in \mathbb{R}$. Для этого достаточно, чтобы его дискриминант был не положителен:
$$
D_4 = (2(x, y))^2 - 4(x, x)(y, y) \leq 0.
$$
Упрощаем:
$$
4[(x, y)^2 - (x, x)(y, y)] \leq 0 \quad \Rightarrow \quad (x, y)^2 \leq (x, x)(y, y).
$$
Извлекая корень, получаем требуемое неравенство:
$$
|(x, y)| \leq \sqrt{(x, x)(y, y)}.
$$

\subsection*{Доказательство}
Начнём с известной вам геометрической ситуации. Пусть $\vec{a}$ и $\vec{b}$ — два произвольных вектора на плоскости. Скалярное произведение данных векторов по абсолютной величине не превосходит произведения их длин:
$$
|\vec{a} \cdot \vec{b}| \leq |\vec{a}| |\vec{b}|,
$$
поскольку $\vec{a} \cdot \vec{b} = |\vec{a}| |\vec{b}| \cos \varphi$, где $\varphi$ — угол между векторами.

Теперь запишем это неравенство в координатах. Рассмотрим на плоскости прямоугольную систему координат, пусть $\vec{i}$ и $\vec{j}$ — единичные базисные векторы, направленные вдоль координатных осей. Тогда векторы $\vec{a}$ и $\vec{b}$ можно разложить по базисным векторам:
$$
\vec{a} = a_1 \vec{i} + a_2 \vec{j}, \quad \vec{b} = b_1 \vec{i} + b_2 \vec{j}.
$$
причём длины векторов и скалярное произведение выражаются через координаты векторов — числа $a_1, a_2, b_1, b_2$ — следующим образом:
$$
|\vec{a}| = \sqrt{a_1^2 + a_2^2}, \quad |\vec{b}| = \sqrt{b_1^2 + b_2^2}, \quad \vec{a} \cdot \vec{b} = a_1 b_1 + a_2 b_2.
$$
В результате наше неравенство примет вид
$$
|a_1 b_1 + a_2 b_2| \leq \sqrt{a_1^2 + a_2^2} \cdot \sqrt{b_1^2 + b_2^2},
$$
или
$$
(a_1^2 + a_2^2)(b_1^2 + b_2^2) \geq (a_1 b_1 + a_2 b_2)^2.
$$
Это и есть неравенство Коши — Буняковского — Шварца (КБШ) в простейшем случае $n = 2$. Равенство достигается тогда и только тогда, когда координаты векторов пропорциональны: $a_1/b_1 = a_2/b_2$. Действительно, пропорциональность координат равносильна коллинеарности векторов $\vec{a}$ и $\vec{b}$, то есть условию $\cos \varphi = \pm 1$ или $\vec{a} \cdot \vec{b} = \pm |\vec{a}| |\vec{b}|$.

Если провести аналогичные рассуждения с векторами в трёхмерном пространстве, то получится неравенство КБШ для $n = 3$:
$$
(a_1^2 + a_2^2 + a_3^2)(b_1^2 + b_2^2 + b_3^2) \geq (a_1 b_1 + a_2 b_2 + a_3 b_3)^2.
$$
Равенство достигается при $a_1/b_1 = a_2/b_2 = a_3/b_3$.
В общем случае неравенство Коши — Буняковского — Шварца имеет вид
$$
(a_1^2 + a_2^2 + \dots + a_n^2)(b_1^2 + b_2^2 + \dots + b_n^2) \geq (a_1 b_1 + a_2 b_2 + \dots + a_n b_n)^2
$$
для любых двух наборов действительных чисел $a_1, a_2, \dots, a_n$ и $b_1, b_2, \dots, b_n$, причём равенство достигается тогда и только тогда, когда эти наборы пропорциональны: $\dfrac{a_1}{b_1} = \dfrac{a_2}{b_2} = \dots = \dfrac{a_n}{b_n}$ (в записи пропорциональности мы допускаем ноль в знаменателе, когда ноль присутствует и в соответствующем числите).

Для положительных чисел $x_1, \dots, x_n, y_1, \dots, y_n$ неравенство КБШ можно переписать в следующем виде:
$$
(x_1 + \dots + x_n)(y_1 + \dots + y_n) \geq \left( \sqrt{x_1 y_1} + \dots + \sqrt{x_n y_n} \right)^2.
$$

\subsection*{Неравенство треугольника}
Пусть $(X, \|\cdot\|)$ — нормированное векторное пространство, где $X$ — произвольное множество, а $\|\cdot\|$ — определённая на $X$ норма. Тогда по определению последней справедливо:
$$
\|x + y\| \leq \|x\| + \|y\|, \quad \forall x, y \in X.
$$
\subsection*{Доказательство неравенства треугольника (с использованием свойств скалярного произведения)}
\begin{enumerate}
    \item Вычислим квадрат нормы суммы:
    $$
    \|\vec{u} + \vec{v}\|^2 = \langle \vec{u} + \vec{v}, \vec{u} + \vec{v} \rangle.
    $$
    Раскроем скалярное произведение:
    $$
    \|\vec{u} + \vec{v}\|^2 = \langle \vec{u}, \vec{u} \rangle + 2\langle \vec{u}, \vec{v} \rangle + \langle \vec{v}, \vec{v} \rangle = \|\vec{u}\|^2 + 2\langle \vec{u}, \vec{v} \rangle + \|\vec{v}\|^2.
    $$
    
    \item Применим неравенство Коши-Буняковского:  
    В евклидовом пространстве выполняется $|\langle \vec{u}, \vec{v} \rangle| \leq \|\vec{u}\| \cdot \|\vec{v}\|$. Следовательно:
    $$
    \langle \vec{u}, \vec{v} \rangle \leq |\langle \vec{u}, \vec{v} \rangle| \leq \|\vec{u}\| \cdot \|\vec{v}\|.
    $$
    Подставляем в выражение:
    $$
    \|\vec{u} + \vec{v}\|^2 \leq \|\vec{u}\|^2 + 2\|\vec{u}\| \cdot \|\vec{v}\| + \|\vec{v}\|^2.
    $$
    
    \item Преобразуем в полный квадрат:
    $$
    \|\vec{u} + \vec{v}\|^2 \leq (\|\vec{u}\| + \|\vec{v}\|)^2.
    $$
    
    \item Извлекаем квадратный корень:  
    Так как нормы неотрицательны, получаем:
    $$
    \|\vec{u} + \vec{v}\| \leq \|\vec{u}\| + \|\vec{v}\|.
    $$
\end{enumerate}


\subsection*{Теорема Пифагора в евклидовых пространствах (не уверен что это именно то что нужно)}
Если векторы $\vec{u}$ и $\vec{v}$ ортогональны (т.е. их скалярное произведение равно нулю: $\langle \vec{u}, \vec{v} \rangle = 0$), то выполняется:
$$
\|\vec{u} + \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2,
$$
где $\|x\| = \sqrt{\langle x, x \rangle}$ — евклидова норма.

\subsection*{Доказательство}
\begin{enumerate}
    \item Раскрываем квадрат нормы суммы:
    $$
    \|\vec{u} + \vec{v}\|^2 = \langle \vec{u} + \vec{v}, \vec{u} + \vec{v} \rangle.
    $$
    \item Используем свойства скалярного произведения (билинейность и симметричность):
    $$
    \langle \vec{u} + \vec{v}, \vec{u} + \vec{v} \rangle = \langle \vec{u}, \vec{u} \rangle + \langle \vec{u}, \vec{v} \rangle + \langle \vec{v}, \vec{u} \rangle + \langle \vec{v}, \vec{v} \rangle.
    $$
    \item Подставляем условие ортогональности ($\langle \vec{u}, \vec{v} \rangle = \langle \vec{v}, \vec{u} \rangle = 0$):
    $$
    \|\vec{u} + \vec{v}\|^2 = \langle \vec{u}, \vec{u} \rangle + 0 + 0 + \langle \vec{v}, \vec{v} \rangle = \|\vec{u}\|^2 + \|\vec{v}\|^2.
    $$
\end{enumerate}

\subsection*{Теорема косинусов в евклидовом пространстве}
Для любых векторов $\vec{u}$ и $\vec{v}$ с углом $\theta$ между ними:
$$
\|\vec{u} - \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2 - 2\|\vec{u}\| \cdot \|\vec{v}\| \cos \theta.
$$

\subsection*{Доказательство теоремы косинусов}
\begin{enumerate}
    \item Раскроем квадрат нормы разности векторов через скалярное произведение:
    $$
    \|\vec{u} - \vec{v}\|^2 = \langle \vec{u} - \vec{v}, \vec{u} - \vec{v} \rangle.
    $$
    \item Используем билинейность и симметричность скалярного произведения:
    $$
    \langle \vec{u} - \vec{v}, \vec{u} - \vec{v} \rangle = \langle \vec{u}, \vec{u} \rangle - 2\langle \vec{u}, \vec{v} \rangle + \langle \vec{v}, \vec{v} \rangle = \|\vec{u}\|^2 - 2\langle \vec{u}, \vec{v} \rangle + \|\vec{v}\|^2.
    $$
    \item Подставляем определение угла ($\langle \vec{u}, \vec{v} \rangle = \|\vec{u}\| \cdot \|\vec{v}\| \cos \theta$):
    $$
    \|\vec{u} - \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2 - 2\|\vec{u}\| \cdot \|\vec{v}\| \cos \theta.
    $$
\end{enumerate}


\subsection*{Следствия}
\begin{enumerate}
    \item \textbf{Теорема Пифагора} (при $\theta = 90^\circ$):
    $$
    \|\vec{u} - \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2.
    $$
    \item \textbf{Вычисление угла}:
    $$
    \cos \theta = \frac{\|\vec{u}\|^2 + \|\vec{v}\|^2 - \|\vec{u} - \vec{v}\|^2}{2\|\vec{u}\| \cdot \|\vec{v}\|}.
    $$
    \item \textbf{Критерий ортогональности}:
    $$
    \|\vec{u}\|^2 + \|\vec{v}\|^2 = \|\vec{u} - \vec{v}\|^2 \iff \vec{u} \perp \vec{v}.
    $$
\end{enumerate}

\textit{Теорема верна для всех евклидовых пространств: $\mathbb{R}^n$, пространств матриц, многочленов, функций.}

\section{Ортогональность векторов. Ортогональное дополнение к подпространству, его свойства. Ортогональная проекция и ортогональная составляющая. Ортонормированные базисы и ортогональные матрицы.}

\subsection*{Определения}
\underline{Определение}: Элементы $x, y \in E_n$ называются \textbf{ортогональными}, если  
$$
(x, y) = 0.
$$

\underline{Определение}: Базис $\{e_i\}$ в $E_n$ называется \textbf{ортонормированным}, если  
$$
(e_i, e_k) = \delta_{ik}, \quad \text{где } \delta_{ik} = 
\begin{cases} 
1, & i = k, \\ 
0, & i \neq k 
\end{cases}
$$
— символ Кронекера.

\subsection*{Ортогональное дополнение}
\underline{Определение}: Пусть $V$ — векторное пространство над полем $F$ с билинейной формой $B$. Вектор $u$ ортогонален слева вектору $v$, а вектор $v$ ортогонален справа вектору $u$ тогда и только тогда, когда $B(u, v) = 0$. Левое ортогональное дополнение подпространства $W$ — это множество векторов $x \in V$, ортогональных слева каждому вектору $y \in W$:
$$
W^\perp = \{x \in V : B(x, y) = 0 \ \forall y \in W\}.
$$

\subsection*{Свойства ортогонального дополнения}

\begin{enumerate}
    \item Ортогональное дополнение является подпространством, то есть замкнуто относительно сложения векторов и умножения на элемент поля.
    \item Если $X \subseteq Y$, то $Y^\perp \subseteq X^\perp$.
    \item Если форма $B$ является невырожденной, а пространство $V$ конечномерно, то  
    $$
    \dim W + \dim W^\perp = \dim V.
    $$
    \item Если же $V$ — конечномерное евклидово пространство и $B$ — скалярное произведение (или же унитарное пространство и эрмитово скалярное произведение соответственно), то для любого подпространства $W \subseteq V$ выполняется разложение в прямую сумму:
    $$
    V = W \oplus W^\perp.
    $$
\end{enumerate}

\subsection*{Ортогональная проекция и ортогональная составляющая}
В евклидовом пространстве $V$ со скалярным произведением $\langle \cdot, \cdot \rangle$ и подпространством $W \subseteq V$:

\begin{enumerate}
    \item \textbf{Ортогональная проекция}:  
    Обозначается $\mathrm{proj}_W(\vec{y})$. Это вектор $\hat{\vec{y}} \in W$, для которого:
    $$
    \vec{y} - \hat{\vec{y}} \perp W \quad (\text{т.е. } \langle \vec{y} - \hat{\vec{y}}, \vec{w} \rangle = 0 \quad \forall \vec{w} \in W).
    $$
    
    \item \textbf{Ортогональная составляющая}:  
    Обозначается $\mathrm{ort}_W(\vec{y})$. Это вектор:
    $$
    \vec{z} = \vec{y} - \hat{\vec{y}}, \quad \text{где } \vec{z} \perp W.
    $$
\end{enumerate}

\textit{Замечание:} Вектор $\vec{y}$ принадлежит всему пространству $V$, а его проекция $\hat{\vec{y}}$ и составляющая $\vec{z}$ разделяют $V$ на подпространство $W$ и его ортогональное дополнение $W^\perp$.

\subsection*{Ключевые свойства}
\begin{enumerate}
    \item \textbf{Единственность разложения}:  
    $$
    \vec{y} = \hat{\vec{y}} + \vec{z}, \quad \hat{\vec{y}} \in W, \ \vec{z} \in W^\perp.
    $$
    
    \item \textbf{Минимальное расстояние}:  
    $\hat{\vec{y}}$ — ближайший к $\vec{y}$ вектор в $W$:  
    $$
    \|\vec{y} - \hat{\vec{y}}\| = \min_{\vec{w} \in W} \|\vec{y} - \vec{w}\|.
    $$
    
    \item \textbf{Теорема Пифагора}:  
    $$
    \|\vec{y}\|^2 = \|\hat{\vec{y}}\|^2 + \|\vec{z}\|^2.
    $$
\end{enumerate}

\subsection*{Формулы для вычисления}
\begin{enumerate}
    \item \textbf{Проекция на вектор $\vec{u}$}:  
    Если $W = \mathrm{span}(\vec{u})$, то  
    $$
    \hat{\vec{y}} = \frac{\langle \vec{y}, \vec{u} \rangle}{\langle \vec{u}, \vec{u} \rangle} \vec{u}, \quad \vec{z} = \vec{y} - \hat{\vec{y}}.
    $$
    
    \item \textbf{Проекция на подпространство с ортонормированным базисом}:  
    Если $\{\vec{u}_1, \dots, \vec{u}_k\}$ — ортонормированный базис $W$, то  
    $$
    \hat{\vec{y}} = \sum_{i=1}^k \langle \vec{y}, \vec{u}_i \rangle \vec{u}_i, \quad \vec{z} = \vec{y} - \hat{\vec{y}}.
    $$
\end{enumerate}

\subsection*{Ортогональный и ортонормированный базисы}
\textbf{Ортогональный базис} — это базис, составленный из попарно ортогональных векторов.  
\textbf{Ортонормированный базис} удовлетворяет ещё и условию единичности нормы всех его элементов. То есть это ортогональный базис, где каждый вектор имеет длину 1.

Последнее удобно записывается при помощи \textit{символа Кронекера}:
$$
\langle e_i, e_j \rangle = \delta_{ij},
$$
где  
$$
\delta_{ij} = 
\begin{cases} 
1, & i = j, \\ 
0, & i \neq j 
\end{cases}
$$
— символ Кронекера.  

Это означает, что скалярное произведение любой пары базисных векторов равно нулю, когда они не совпадают ($i \neq j$), и равно единице при совпадающем индексе ($i = j$).

\subsection*{Ортогональная матрица}
\textbf{Ортогональная матрица} $A$ — квадратная матрица с вещественными элементами, для которой выполняется равенство:  
$$
AA^T = A^T A = E,
$$  
где $A^T$ — транспонированная матрица, $E$ — единичная матрица. Это эквивалентно условию:  
$$
A^{-1} = A^T.
$$  

\textit{Комплексным аналогом ортогональной матрицы} является \textbf{унитарная матрица}.  
Ортогональная матрица с определителем $+1$ называется \textbf{специальной ортогональной}.

\subsection*{Канонический вид ортогональной матрицы}
Любая вещественная ортогональная матрица подобна блочно-диагональной матрице следующего вида:
$$
\begin{pmatrix}
\pm 1 & & & \\
& \ddots & & \\
& & \cos \varphi & \sin \varphi \\
& & -\sin \varphi & \cos \varphi
\end{pmatrix},
$$
где блоки $\pm 1$ соответствуют отражениям, а блоки 
$$
\begin{pmatrix}
\cos \varphi & \sin \varphi \\
-\sin \varphi & \cos \varphi
\end{pmatrix}
$$ 
— поворотам на угол $\varphi$. Это следует из теоремы о приведении ортогональной матрицы к каноническому виду через ортогональную эквивалентность.

\subsection*{Связь с унитарными и нормальными матрицами}
Ортогональная матрица $Q$ является \textbf{унитарной}, так как выполняется условие:  
$$
Q^{-1} = Q^*,Add commentMore actions
$$  
где $Q^*$ — сопряжённая транспонированная матрица. Это следует из равенства $Q^{-1} = Q^T$ для ортогональных матриц ($Q^T = Q^*$ в вещественном случае).

Кроме того, ортогональная матрица является \textbf{нормальной}, так как:  
$$
QQ^* = Q^*Q.
$$  
Для ортогональной матрицы это равенство принимает вид:  
$$
QQ^T = Q^T Q = E,
$$  
что тривиально выполняется.

\section{Матрица и определитель Грама системы векторов евклидова пространства, их свойства. Процесс ортогонализации Грама-Шмидта, его геометрический смысл. QR-разложение.}

\subsection*{Матрица Грама}
\textbf{Определение}: Пусть в евклидовом пространстве $\mathbb{E}$ задано скалярное произведение $\langle \cdot, \cdot \rangle$. Матрицей Грама системы векторов $\{X_1, \dots, X_m\}$ называется квадратная матрица:
$$
G(X_1, \dots, X_m) = 
\begin{pmatrix}
\langle X_1, X_1 \rangle & \langle X_1, X_2 \rangle & \dots & \langle X_1, X_m \rangle \\
\langle X_2, X_1 \rangle & \langle X_2, X_2 \rangle & \dots & \langle X_2, X_m \rangle \\
\vdots & \vdots & \ddots & \vdots \\
\langle X_m, X_1 \rangle & \langle X_m, X_2 \rangle & \dots & \langle X_m, X_m \rangle
\end{pmatrix}
= [G_{jk}]_{j,k=1}^m,
$$
где $G_{jk} = \langle X_j, X_k \rangle$.

\subsection*{Ключевые свойства}

\begin{enumerate}
    \item \textbf{Симметричность}:
    \begin{enumerate}
        \item В вещественных пространствах: $G^\top = G$;
        \item В комплексных: $G = G^*$ (эрмитовость).
    \end{enumerate}
    \item \textbf{Положительная полуопределённость}:
        Для любого вектора $x \in \mathbb{R}^m$:  
        $$
        x^\top G x \geq 0.
        $$  
        Равенство нулю $\Leftrightarrow$ линейная зависимость векторов $\{X_1, \dots, X_m\}$.
    \item \textbf{Связь с линейной независимостью}:
        Векторы $X_1, \dots, X_m$ линейно независимы $\Leftrightarrow \det(G) > 0$.  
        Определитель матрицы Грама называется \textit{грамианом}.
    \item \textbf{Инвариантность относительно базиса}:
        Значения $\langle X_i, X_j \rangle$ не зависят от выбора базиса пространства $\mathbb{E}$.
    \item \textbf{Представление через матрицу}:
        Если $A$ — матрица, столбцы которой содержат координаты векторов $X_i$ в ортонормированном базисе, то:
        $$
        G = A^\top A \quad (\text{в вещественном случае}), \qquad G = A^* A \quad (\text{в комплексном случае}).
        $$
    \item \textbf{Геометрическая интерпретация}:
    \begin{enumerate}
        \item Диагональные элементы: $G_{ii} = \langle X_i, X_i \rangle = \|X_i\|^2$ (квадрат длины вектора $X_i$);
        \item Внедиагональные элементы: $G_{ij} = \|X_i\| \cdot \|X_j\| \cdot \cos \theta_{ij}$, где $\theta_{ij}$ — угол между $X_i$ и $X_j$;
        \item Определитель $G$: квадрат объема $m$-мерного параллелепипеда, натянутого на векторы $X_1, \dots, X_m$.
    \end{enumerate}
\end{enumerate}
        
\subsection*{Грамиан}
\textbf{Определение}: Определителем Грама (грамианом) системы векторов $\{\vec{e}_1, \vec{e}_2, \dots, \vec{e}_n\}$ в евклидовом пространстве называется определитель матрицы Грама:
$$
\begin{vmatrix}
\langle \vec{e}_1, \vec{e}_1 \rangle & \langle \vec{e}_1, \vec{e}_2 \rangle & \dots & \langle \vec{e}_1, \vec{e}_n \rangle \\
\langle \vec{e}_2, \vec{e}_1 \rangle & \langle \vec{e}_2, \vec{e}_2 \rangle & \dots & \langle \vec{e}_2, \vec{e}_n \rangle \\
\vdots & \vdots & \ddots & \vdots \\
\langle \vec{e}_n, \vec{e}_1 \rangle & \langle \vec{e}_n, \vec{e}_2 \rangle & \dots & \langle \vec{e}_n, \vec{e}_n \rangle
\end{vmatrix},
$$
где $\langle \vec{e}_i, \vec{e}_j \rangle$ — скалярное произведение векторов $\vec{e}_i$ и $\vec{e}_j$.

\textit{Свойство}: Грамиан равен нулю тогда и только тогда, когда система векторов $\{\vec{e}_1, \dots, \vec{e}_n\}$ линейно зависима.

\subsection*{Основные свойства и интерпретации}

\begin{enumerate}
    \item \textbf{Критерий линейной независимости}:  
    $$
    \begin{aligned}
    \det(G) > 0 &\iff \text{векторы } \vec{v}_1, \dots, \vec{v}_m \text{ линейно независимы}, \\
    \det(G) = 0 &\iff \text{векторы } \vec{v}_1, \dots, \vec{v}_m \text{ линейно зависимы}.
    \end{aligned}
    $$
    \item \textbf{Геометрический смысл (квадрат объёма)}:  
    Для линейно независимых векторов $\vec{v}_1, \dots, \vec{v}_m$ в $\mathbb{R}^n$:  
    $$
    [\mathrm{Vol}_m(\vec{v}_1, \dots, \vec{v}_m)]^2 = \det(G),
    $$  
    где $\mathrm{Vol}_m$ — объём $m$-мерного параллелепипеда, натянутого на векторы ($m \leq n$).
    \item \textbf{Связь с координатами}:  
    Если векторы заданы в ортонормированном базисе матрицей $A$ (столбцы — координаты векторов), то при $m = n$:  
    $$
    \det(G) = [\det(A)]^2.
    $$
    \item \textbf{Геометрическая интерпретация для малых размерностей}:
    \begin{enumerate}
        \item \textit{Один вектор} ($m = 1$):  
        $$
        G = [\langle \vec{v}_1, \vec{v}_1 \rangle] = [\|\vec{v}_1\|^2] \implies \det(G) = \|\vec{v}_1\|^2.
        $$  
        → Квадрат длины вектора.
        \item \textit{Два вектора} ($m = 2$):  
        $$
        G = 
        \begin{pmatrix}
        \langle \vec{v}_1, \vec{v}_1 \rangle & \langle \vec{v}_1, \vec{v}_2 \rangle \\
        \langle \vec{v}_2, \vec{v}_1 \rangle & \langle \vec{v}_2, \vec{v}_2 \rangle
        \end{pmatrix}, \quad
        \det(G) = \|\vec{v}_1\|^2 \cdot \|\vec{v}_2\|^2 - \langle \vec{v}_1, \vec{v}_2 \rangle^2 = (\|\vec{v}_1\| \cdot \|\vec{v}_2\| \cdot \sin \theta)^2.
        $$  
        → Квадрат площади параллелограмма.
        \item \textit{Три вектора} ($m = 3$):  
        $$
        \det(G) = (\vec{v}_1 \times \vec{v}_2) \cdot \vec{v}_3 \quad \text{(смешанное произведение)}.
        $$  
        → Квадрат объёма параллелепипеда.
    \end{enumerate}
\end{enumerate}
        
\subsection*{Классический процесс Грамма — Шмидта}

\subsubsection*{Алгоритм}
Пусть имеются линейно независимые векторы $\vec{a}_1, \dots, \vec{a}_n$, и пусть $\mathrm{proj}_{\vec{b}} \vec{a}$ — оператор проекции вектора $\vec{a}$ на вектор $\vec{b}$, определённый как  
$$
\mathrm{proj}_{\vec{b}} \vec{a} = \frac{\langle \vec{a}, \vec{b} \rangle}{\langle \vec{b}, \vec{b} \rangle} \vec{b},
$$  
где $\langle \vec{a}, \vec{b} \rangle$ — скалярное произведение векторов $\vec{a}$ и $\vec{b}$.

Классический процесс Грамма — Шмидта выполняется следующим образом:
\begin{align*}
\vec{b}_1 &= \vec{a}_1 & (1) \\
\vec{b}_2 &= \vec{a}_2 - \mathrm{proj}_{\vec{b}_1} \vec{a}_2 & (2) \\
\vec{b}_3 &= \vec{a}_3 - \mathrm{proj}_{\vec{b}_1} \vec{a}_3 - \mathrm{proj}_{\vec{b}_2} \vec{a}_3 & (3) \\
&\vdots \\
\vec{b}_n &= \vec{a}_n - \mathrm{proj}_{\vec{b}_1} \vec{a}_n - \mathrm{proj}_{\vec{b}_2} \vec{a}_n - \cdots - \mathrm{proj}_{\vec{b}_{n-1}} \vec{a}_n & (n)
\end{align*}

На основе каждого вектора $\vec{b}_j$ ($j = 1, \dots, n$) может быть получен нормированный вектор $\vec{e}_j$ единичной длины, определённый как  
$$
\vec{e}_j = \frac{\vec{b}_j}{\|\vec{b}_j\|}.
$$

\textbf{Результаты процесса Грамма — Шмидта}:  
- $\vec{b}_1, \dots, \vec{b}_n$ — система ортогональных векторов;  
- $\vec{e}_1, \dots, \vec{e}_n$ — система ортонормированных векторов.  

Вычисление $\vec{b}_1, \dots, \vec{b}_n$ называется \textit{ортогонализацией Грамма — Шмидта}, а $\vec{e}_1, \dots, \vec{e}_n$ — \textit{ортонормализацией Грамма — Шмидта}.


здесь геометрическая интерпретация, картинки я вставить не смог, НО \href{https://ru.wikipedia.org/wiki/Процесс_Грама_―_Шмидта}{здесь} хорошо в картинках показано как и зачем:


\subsection*{QR-разложение матрицы}
\textbf{QR-разложение} — это представление матрицы $A$ в виде произведения ортогональной матрицы $Q$ и верхнетреугольной матрицы $R$:  
$$
A = QR.
$$

\subsection*{Общий алгоритм через процесс Грамма–Шмидта}
Пусть $A \in \mathbb{R}^{n \times m}$ ($n \geq m$) — матрица с линейно независимыми столбцами $\vec{a}_1, \vec{a}_2, \dots, \vec{a}_m$.  
Алгоритм QR-разложения:
\begin{enumerate}
    \item \textbf{Ортогонализация}: Применить процесс Грамма–Шмидта к столбцам матрицы $A$:  
    $$
    \begin{aligned}
    \vec{u}_1 &= \vec{a}_1, \quad \vec{e}_1 = \frac{\vec{u}_1}{\|\vec{u}_1\|}, \\
    \vec{u}_2 &= \vec{a}_2 - \langle \vec{a}_2, \vec{e}_1 \rangle \vec{e}_1, \quad \vec{e}_2 = \frac{\vec{u}_2}{\|\vec{u}_2\|}, \\
    &\vdots \\
    \vec{u}_k &= \vec{a}_k - \sum_{i=1}^{k-1} \langle \vec{a}_k, \vec{e}_i \rangle \vec{e}_i, \quad \vec{e}_k = \frac{\vec{u}_k}{\|\vec{u}_k\|}.
    \end{aligned}
    $$
    Полученные векторы $\vec{e}_1, \dots, \vec{e}_m$ образуют ортонормированный базис.
    
    \item \textbf{Формирование матрицы $Q$}:  
    Столбцы матрицы $Q \in \mathbb{R}^{n \times m}$ — ортонормированные векторы $\vec{e}_1, \dots, \vec{e}_m$:  
    $$
    Q = \begin{bmatrix} 
    \vec{e}_1 & \vec{e}_2 & \cdots & \vec{e}_m 
    \end{bmatrix}.
    $$
    Матрица $Q$ удовлетворяет условию $Q^\top Q = I_m$, где $I_m$ — единичная матрица размера $m \times m$.
    
    \item \textbf{Вычисление матрицы $R$}:  
    Верхнетреугольная матрица $R \in \mathbb{R}^{m \times m}$ вычисляется как $R = Q^\top A$:  
    $$
    R = 
    \begin{bmatrix}
    \langle \vec{a}_1, \vec{e}_1 \rangle & \langle \vec{a}_2, \vec{e}_1 \rangle & \cdots & \langle \vec{a}_m, \vec{e}_1 \rangle \\
    0 & \langle \vec{a}_2, \vec{e}_2 \rangle & \cdots & \langle \vec{a}_m, \vec{e}_2 \rangle \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \langle \vec{a}_m, \vec{e}_m \rangle
    \end{bmatrix}.
    $$
\end{enumerate}

\subsection*{Свойства QR-разложения}

\begin{enumerate}
    \item Матрица $Q$ ортогональна: $Q^\top Q = I_m$.
    \item Матрица $R$ — верхнетреугольная, её диагональные элементы положительны, если $\vec{u}_k \neq 0$.
    \item Если $A$ — квадратная невырожденная матрица, то $Q$ также квадратная и ортогональная ($Q^\top = Q^{-1}$).
    \item QR-разложение существует для любой матрицы $A \in \mathbb{R}^{n \times m}$ ($n \geq m$) с полным рангом.
\end{enumerate}

\section{Расстояние и угол между вектором и подпространством}

\subsection{Расстояние от вектора до подпространства}
Расстоянием от вектора $x$ до подпространства $L$ называется:
\[
d(x, L) = \min_{y \in L} \|x - y\|
\]
Минимум достигается при $y = \operatorname{proj}_L(x)$, поэтому:
\[
d(x, L) = \|x - \operatorname{proj}_L(x)\| = \|\operatorname{ort}_L(x)\|
\]
Таким образом, расстояние — это длина ортогональной составляющей.

\subsection{Метрика в пространстве}
Пусть $V$ — векторное пространство, а $\rho: V \times V \to \mathbb{R}$ — функция, определяющая расстояние между векторами:
\[
\rho(x, y) = \|x - y\|
\]
Свойства метрики $\rho$:
\begin{enumerate}
    \item $\rho(x, y) = \rho(y, x)$
    \item $\rho(x, y) \geq 0$ и $\rho(x, x) = 0$
    \item $\rho(x, y) \leq \rho(x, t) + \rho(t, y)$
\end{enumerate}
Расстояние между множествами $A$ и $B$:
\[
\rho(A, B) = \inf \{ \rho(x, y) \mid x \in A, y \in B \}
\]

\textbf{Теорема.}

Пусть \( U \) — подпространство векторного пространства \( V \), а \( x \in V \). Тогда

$\rho (x, U) = |ort_U(x)|$ и $pr_U(x)$ — единственный ближайший к $x$ элемент $U$.

\textbf{Доказательство.}

Пусть $y \in U$:
\[
\rho (x - y)^2 = (x - y, x-y) = |x-y|^2 = |\operatorname{ort}_U(x) + \operatorname{pr}_U(x) - y|^2 = 
\]
\[
= |(pr_U(X) - y) + ort_U(x)|^2 = |pr_U(x) - y|^2 + |ort_U(x)|^2 \geq |ort_U(x)|^2; 
\]

и достигается при $y = pr_U(x) \ \ \square$

\subsection{Угол между вектором и подпространством}
Углом $\varphi$ между вектором $x$ и подпространством $L$ называется угол между $x$ и его проекцией $\operatorname{proj}_L(x)$. Формула:
\[
\cos \varphi = \frac{\|\operatorname{proj}_L(x)\|}{\|x\|}
\]
\[
\sin \angle(x, L) = \frac{\|\operatorname{ort}_L(x)\|}{\|x\|}
\]
\[
\cos \angle(x, L) = \frac{(x, u)}{\|x\| \|u\|}, \quad u \in L, \; u \perp \operatorname{pr}_L(x)
\]

\textbf{Доказательство формулы для угла:}
Пусть $x$ — произвольный вектор, а $U$ — подпространство. Разложим $x$ по базису подпространства $U$ и его дополнения:
\[
x = \operatorname{pr}_U(x) + \operatorname{ort}_U(x),
\]
где $\operatorname{pr}_U(x) \in U$ — проекция $x$ на $U$, а $\operatorname{ort}_U(x) \in U^\perp$ — ортогональная составляющая.

Выберем произвольный вектор $u \in U$ и разложим его:
\[
u = \lambda \operatorname{pr}_U(x) + \mu u^\perp,
\]
где $u^\perp \in U^\perp$ — ортогональный компонент.

Тогда скалярное произведение $(x, u)$ вычисляется следующим образом:
\[
(x, u) = (\operatorname{pr}_U(x) + \operatorname{ort}_U(x), \lambda \operatorname{pr}_U(x) + \mu u^\perp) = \lambda \|\operatorname{pr}_U(x)\|^2.
\]

Норма вектора $u$ равна:
\[
\|u\| = \sqrt{\|\lambda \operatorname{pr}_U(x)\|^2 + \|\mu u^\perp\|^2}.
\]

Теперь вычислим косинус угла:
\[
\cos \angle(x, U) = \frac{(x, u)}{\|x\| \|u\|} = \frac{\lambda \|\operatorname{pr}_U(x)\|^2}{\|x\| \sqrt{(\lambda \operatorname{pr}_U(x))^2 + (\mu u^\perp)^2}}.
\]

Но $(\mu u^\perp)^2$ = 0, тогда получим:

\[
\cos \angle(x, U) = \frac{\lambda \|\operatorname{pr}_U(x)\|^2}{\|x\| \|\lambda \operatorname{pr}_U(x)\|} = \frac{\|\operatorname{pr}_U(x)\|}{\|x\|}. \ \ \square
\]

\section{Комплексификация евклидова пространства эрмитово пространство. Эрмитово скалярное произведение, его свойства. Неравенства Коши-Буняковского и треугольника в эрмитовом пространстве.}

\subsection{Комплексификация евклидова пространства}
Пусть $V$ — вещественное евклидово пространство с скалярным произведением $(\cdot, \cdot): V \times V \to \mathbb{R}$.  
Комплексификацией $V$ называется комплексное линейное пространство:
\[
V_\mathbb{C} = V \otimes \mathbb{C}
\]
Элементы $V_\mathbb{C}$ имеют вид $x + iy$, где $x, y \in V$.

На $V_\mathbb{C}$ можно ввести эрмитово скалярное произведение:
\[
(a + bi, c + di)_\mathbb{C} = (a, c) + (b, d) + i((b, c) - (a, d)),
\]

Оно положительно определено:

\[
(z, z)_\mathbb{C} = (a + вi, a + bi)_\mathbb{C} = (a, a) + (b, b) \geq 0
\] и = 0 \Longleftrightarrow z = 0

\subsection{Свойства эрмитова скалярного произведения}

Докажем основные свойства эрмитова скалярного произведения:

\subsubsection{Эрмитова симметричность:}
\[
(x, y)_\mathbb{C} = \overline{(y, x)_\mathbb{C}}
\]

Пусть $x = a + bi$ и $y = c + di$, где $a, b, c, d \in V$. Тогда:
\[
(x, y)_\mathbb{C} = (a + bi, c + di)_\mathbb{C} = (a, c) + (b, d) + i((b, c) - (a, d)).
\]

Аналогично:
\[
(y, x)_\mathbb{C} = (c + di, a + bi)_\mathbb{C} = (c, a) + (d, b) + i((d, a) - (c, b)).
\]
Отсюда следует доказываемое. $ \ \square$

\subsubsection{Аддитивность по первому аргументу (по второму аналогично):}
\[
((a_1 + b_1i) + (a_2 + b_2i), c + di) = ((a_1 + a_1) + (b_1 + b_2)i, c + di) = (a_1 + a_2, c) + (b_1 + b_2, d) + i((b_1 + b_2, c) - (a_1+a_2, d)) = \]
\[ 
= (a_1, c) + (a_2, c) + (b_1,d) + (b_2,d) + i((b_1,c)+(b_2,c) - (a_1,d) - (a_2, d)) = (a_1+b_1i, c+di)+(a_2+b_2i,c+di) \ \ \square
\]

\subsubsection{Однородность по первому аргументу:}
\[
(a+bi, (q+wi)(c+di))_\mathbb{C}= (a+bi, (qc - wd) + i(qd + wc)) = (a, qc - wd) + (b, qd + wc) + i((b, qc - wd) - (a, qd + wc))=
\]
\[
= q(a,c)- w(a,d)+ q(b,d)+w(b,c) + i (q(b,c) - w(b,d) - q(a,d) - w (a,c))=
(q-wi)(a+bi, c+di)
\]
Но полуоднородность по второму аргументу: $(x, \alpha y)_\mathbb{C} = \overline{\alpha}(x,y)_\mathbb{C}$ \ \ \square

$\beta \in BL(V_\mathbb{C})$ — полулинейная форма

\subsubsection*{Итог:}
$(V_\mathbb{C}, (  \_ , \_ )_\mathbb{C})$ — Эрмитово (унитарное) пространство

\subsection{Неравенство Коши-Буняковского в  эрмитовом пространстве}

Пусть H — эрмитово (унитарное) пространство с эрмитовым скалярным произведением $(*, *) : H\times H \to \mathbb{C}.$ Тогда для любых $x, y \in H$ выполняется следующее неравенство:
\[
|(x,y)| \leq \|x\| * \|y\|
\]

\textbf{Доказательство:}
\[\]
Рассмотрим функцию:
\[
f(t) = \|x + ty\|^2 = (x + ty, x + ty),
\]
где $t \in \mathbb{C}$. Эта функция всегда неотрицательна, так как норма квадратична:
\[
f(t) = (x, x) + t(y, x) + \overline{t}(x, y) + |t|^2(y, y).
\]

Пусть $t = -\frac{(x, y)}{(y, y)}$. Тогда:
\[
f\left(-\frac{(x, y)}{(y, y)}\right) = \|x\|^2 - \frac{|(x, y)|^2}{(y, y)} \geq 0.
\]

Отсюда:
\[
\|x\|^2 \cdot (y, y) \geq |(x, y)|^2,
\]
или:
\[
|(x, y)| \leq \|x\| \cdot \|y\| \ \ \square.
\]

\subsection{Неравенство треугольника}

Для любого эрмитового пространства $H$ с эрмитовым скалярным произведением $(\cdot, \cdot): H \times H \to \mathbb{C}$ и нормой:
\[
\|x\| = \sqrt{(x, x)},
\]
выполняется неравенство треугольника:
\[
\|x + y\| \leq \|x\| + \|y\|.
\]

\textbf{Доказательство:}

Рассмотрим квадрат нормы суммы:
\[
\|x + y\|^2 = (x + y, x + y) = (x, x) + (x, y) + (y, x) + (y, y) = \|x\|^2 + \|y\|^2 + (x, y) + \overline{(x, y)}.
\]

Заметим, что $(x, y) + \overline{(x, y)} = 2 \operatorname{Re}(x, y)$, значит:
\[
\|x + y\|^2 = \|x\|^2 + \|y\|^2 + 2 \operatorname{Re}(x, y).
\]

Так как $\operatorname{Re}(x, y) \leq |(x, y)|$, получаем:
\[
\|x + y\|^2 \leq \|x\|^2 + \|y\|^2 + 2 |(x, y)|.
\]

Применим неравенство Коши-Буняковского:
\[
|(x, y)| \leq \|x\| \cdot \|y\| \implies \|x + y\|^2 \leq \|x\|^2 + \|y\|^2 + 2 \|x\| \|y\| = (\|x\| + \|y\|)^2.
\]

Извлекая корень из обеих частей:
\[
\|x + y\| \leq \|x\| + \|y\|. \ \ \square
\]

\section{Соответствие между операторами и билинейными функциями в евклидовом пространстве. Сопряжённый оператор и его свойства}

\subsection{Построение изоморфизма между оператором и билинейной формой}

Пусть V — конечномерное векторное пространство, $dim V < \infty$, $\beta \in BL(V)$

Рассмотрим отображение: $V \to V^*$, $y \in V \to \beta( $_$ , y)$

$B \in Hom(V, V^*)$ (линейность по второму аргументу)

$y_1 + y2 \to \beta($_$, y_1) + \beta($_$, y_2) = \beta($_$, y_1 + y_2)$

Тогда можно определить отображение:
$\varphi: BL(V) \to \mathrm{Hom}(V, V^*), \ \varphi$ линейно по определению операций в $BL(V)$$ и $\mathrm{Hom}(V,V^*)$ 

$\mathrm{Ker}(\varphi) = {0} \implies \varphi$ инъективно

$dim(Im(\varphi)) = n^2 - 0 = n^2$

$Im(\varphi) = Hom(V,V^*) \implies \varphi$ сюръективно

\implies $\varphi$ — канонический изоморфизм

\textbf{Теорема:}
$BL(V) = Hom(V, V^*)$

Для любого $ x \in V, x \ne 0, X^TBY \ne 0$

Невырожденный $BL(V) = Iso(V,V^*)$

\textbf{Следствие:}
V — евклидово $\implies V = V ^*$

$e_1,...,e_n $ — ортонормированный базис в $V$

$e_1^*,...,e_n^* -$ сопряженный базис в $V^*$

$(e_i, x) = (e_i, x_1e_1 + ...+x_ne_n) = x_i, (e_i,e_j) = \delta _{ij} = e_i^*(e_j)$ и $=e_j(e_i) => e_i^* = e_i$

\subsection{Теорема о каноническом изоморфизме}
Пусть $V$ — конечномерное евклидово пространство. Тогда существует канонический изоморфизм:
\[
BL(V) \cong End(V)
\]

\textbf{Доказательство:}

$\alpha(x,y) = (x, Ay)$

\textbf{Гомоморфизм:}

$(x, (A_1 + A_2)y) = (x, A_1 y) + (x,A_2y) = \alpha_1(x,y)+ \alpha_2(x,y) = (\alpha_1 + \alpha_2)(x,y)

A_1 + A_2 \longleftrightarrow \alpha_1 + \alpha_2 \implies \mathrm{dim}(End(V)) = \mathrm{dim} (BL(V)) = n^2

\textbf{Ядро:}

$A \ne 0$ существует $y \in V, A(y) \ne 0, \alpha(x,y) \ne 0 \implies \alpha \not\equiv 0$ — только нулевые 

Значит, $End(V) \simeq BL(V)$ (канонический) $ \ \square$

\textbf{Следствие.}
 В ортонормированном базисе евклидового пространства М матрица
 $A \in End(V)$ и соответствующая ему билинейная формы $\alpha \in BL(V)$ совпадают.

 $\alpha^* (x,y) = \alpha(y,x)$ — сопряженная билинейная форма 

 $(x, Ay) = (A^*x,y)$ — определение сопряженного оператора в евклидовом пространстве

 \textbf{Лемма.}
 $e$ — ортонормированный базис $\implies [A^*]_e = [A]_e^T$.

 \textbf{Определение.}
 Оператор A в евклидовом пространстве самосопряженный, если 
 
 $A^* = A \implies (x,Ay)= (Ax,y)$.

\textbf{Свойства:}
\begin{enumerate}
    \item $(A+B)^* = A^*+B^*$
    \item $E^* = E$
    \item $(\lambda A)^* = \lambda A^*, \lambda \in \mathbb{R}$
    \item $(AB)^* = B^*A^*$
\end{enumerate}

\section{Ортогональные операторы, их свойства. Канонический вид матрицы ортогонального оператора.
 Собственные значения и собственные векторы ортогонального оператора. Комплексификация
 ортогонального оператора унитарный оператор. Собственные значения и собственные вектора
 унитарного оператора.}

\subsection{Ортогональные операторы, их свойства}

Пусть $ V $ — евклидово пространство с вещественным скалярным произведением $ (\cdot, \cdot) $, и пусть $ A: V \to V $ — линейный оператор. Рассмотрим следующие утверждения:

\begin{enumerate}
    \item $ (Ax, Ay) = (x, y) $ для всех $ x, y \in V $
    \item $ \|Ax\| = \|x\| $ для всех $ x \in V $
    \item $ A \in \operatorname{Aut}(V) $, изоморфизм евклидовых пространств = биективный изоморфизм
    \item $ A^* = A^{-1} $
    \item $ A^T A = I $
\end{enumerate}

\textbf{Доказательство эквивалентности.}

\textbf{1. $ 1 \implies 2 $}

Пусть $ (Ax, Ay) = (x, y) $ для всех $ x, y \in V $. Положим $ y = x $:
$$
(Ax, Ax) = (x, x)
\implies \|Ax\|^2 = \|x\|^2
\implies \|Ax\| = \|x\|
$$

Таким образом, оператор $ A $ сохраняет нормы.

\textbf{2. $ 2 \implies 3 $}
$(\_,\_) \in BL^+(V)$

$(Ax,Ax) = |Ax|^2 \xrightarrow{поляризация} (Ax, Ay) = \frac{|Ax + Ay| - |Ax|^2 - |Ay|^2}{2} = \frac{|x+y|^2 - |x|^2 - |y|^2}{2} = (x,y)$

\textbf{3. $ 1, 2\implies 3 $}

Найдем $x \in KerA$.
$Ax = 0 : |Ax|^2 = |x|^2 = 0 = (x,x) =0 \Rightarrow x = 0$

Инъективность: $\mathrm{Ker}A = {0}$

$\mathrm{dim}(\mathrm{Im}(A)) + \mathrm{dim}(\mathrm{Ker}(A)) = \mathrm{dim}(V)$

Получаем, что $Im(A) \leq V \implies V = Im(A)$ — сюръекция

\textbf{4. $ 3 \implies 2 $}

Изометрия по определению изоморфизма

\textbf{5. $ 1 \Longleftrightarrow 4 $}

$(Ax, Ay) = (x,y)$

$(Ax, Ay) = (x,A^*Ay) = (x, Ey) = (x,y)$

Получаем, что для любых $x,y \in V, ($_$,$_$)$ невырожденная \implies$ A^*A = E \Longleftrightarrow A^* = A^{-1}$

\textbf{6. $ 4 \Longleftrightarrow 5 $}

Пусть $e$ — ортонормированный базис, тогда:

$[A^*]_e = A^T$, получим $A^TA = [A^*A] = [E] \Longleftrightarrow A \in O_n(\mathbb{R})$

$$
\boxed{
\text{Ортогональный оператор — это оператор, удовлетворяющий условиям выше.}
}
$$
\subsection{Собственные значения и собственные векторы ортогонального оператора}
\subsubsection{Собственные значения ортогонального оператора}
Пусть $x$ — собственный вектор $x \in V_\lambda$ \newline
$(Ax, Ax) = (x,x) = (\lambda x, \lambda x) = \lambda^2(x,x) \implies (\lambda^2 - 1)(x,x) = 0,$\newline но $(x,x) \ne 0 \implies \lambda^2 = 1 \implies \lambda = \pm 1$\newline
Если рассматривать комплексную плоскость, то $|\lambda| = 1$ означает, что решения лежат на единичной окружности.

\subsubsection{Собственные векторы ортогонального оператора}
\textbf{Лемма.}\newline
Собственные векторы, соответствующие разным собственным значениям, ортогональны.\newline
\textbf{Доказательство.}

Пусть $U(x) = \lambda x, U(y)= \mu *y $ и $\lambda \ne \mu$

Рассмотрим скалярное произведение:

$(U(x),U(y)) = (\lambda x, \mu x) = \lambda \mu (x,y)$

$(U(x),U(y)) = (x,y)$ в силу ортогональности

Получаем: $(\lambda \mu - 1)(x,y) = 0$

Тогда возможен только один вариант, $(x,y) = 0 \implies $ ортогональны $\square$

\textbf{Лемма.}\newline
Вещественные собственные векторы существуют в вещественном пространстве тогда и только тогда, когда у оператора есть вещественные собственные значения $\pm 1$. Они соответствуют неподвижным точкам ($\lambda = 1$) или точкам, отраженным относительно начала координат ($\lambda = -1$).

\subsection{Канонический вид матрицы ортогонального оператора}
\textbf{Теорема о каноническом виде.}\newline
Для любого ортогонального оператора $U$, действующего в конечномерном вещественном евклидовом пространстве $E$ размерности $n$, существует ортонормированный базис, в котором матрица оператора $U$ имеет следующую блочно-диагональную каноническую форму:
$[U] = $\begin{pmatrix}
    R(\theta_1)&0&0&\cdots&\cdots&\cdots&\cdots&0\\
    0&R(\theta_2)&0&\cdots&\cdots&\cdots&\cdots&0\\
    0&\cdots&\ddots&\cdots&\cdots&\cdots&\cdots&0\\
    0&\cdots&\cdots&R(\theta_k)&\cdots&\cdots&\cdots&0\\
    0&\cdots&\cdots&\cdots &\varepsilon_1&\cdots&\cdots&0\\
    0&\cdots&\cdots& \cdots&\cdots&\varepsilon_2&\cdots&0\\
    0&\cdots&\cdots& \cdots&\cdots&\cdots&\ddots&\vdots\\
    0&0&0& 0&0&0&\cdots&\varepsilon_m\\
\end{pmatrix},

где $\varepsilon_i$ имеет размер $1\times1$ и равен $\pm1$, а блок $R(\theta_k)$ имеет размер $2\times2$ и вид:

$R(\theta_k)$ = \begin{pmatrix}
    \cos(\theta_k) & -\sin(\theta_k)\\
    \sin(\theta_k) & \cos(\theta_k)
\end{pmatrix},

где $\theta_k \in (0, \pi) \cup (\pi, 2\pi)$

Каждый блок $R(\theta_k)$ действует в своей двумерной инвариантной плоскости, в которой оператор $U$ осуществляет поворот на угол $\theta_k$.

Количество блоков k может быть от $0$ до $\frac{n}{2}$.

Каждому блоку $\varepsilon_i = 1$ соответствует одномерное инвариантное подпространство (собственный вектор $x_i$), на котором оператор $U$ действует как тождественное преобразование: $U(x_i) = x_i$.

Каждому блоку $\epsilon_i = -1$ соответствует одномерное инвариантное подпространство (собственный вектор $x_i$), на котором оператор $U$ действует как центральная симметрия (отражение): $U(x_i) = -x_i$.

Количество таких блоков m может быть от $0$ до $n$.

\textbf{Доказательство.}

Индукция по размерности:

База: $n = 1$
$U = \langle x \rangle, x\ne 0$, тогда $|Ax| = |\lambda x|= |\lambda||x| = |x| \implies \lambda = \pm 1 $

$n = 2$

Пусть $e_1, e_2 $ — ортонормированный базис, $A \in O(U):$

$(A|_U)(e_1) = \alpha e_1 + \beta e_2$

$|\alpha e_1 + \beta e_2|^2 = |e_1|^2 = 1 = \alpha^2 + \beta^2$

$(A|_U)(e_2) = \gamma e_1 + \delta e_2$

$|\gamma e_1 + \delta e_2|^2 = |e_1|^2 = 1 = \gamma^2 + \delta^2$

$(A|_U)(e_1) \perp (A|_U)(e_2) => (\alpha e_1 + \beta e_2, \gamma e_1 + \delta e_2) = \alpha\gamma + \beta\delta = 0$

\begin{cases}
    \alpha^2 + \beta^2 = 1 \\
    \gamma^2 + \delta^2 = 1
\end{cases}

Пусть $\alpha, = \cos(\varphi), \beta = \sin(\varphi), \gamma = \cos(\psi), \delta = \sin(\psi)$, тогда:

$\cos(\varphi)\cos(\psi) + \sin(\varphi)\sin(\psi) = 0$

$\cos(\varphi - \psi) = 0 \implies$имеем вид:

$[A|_U]_{e_1,e_2}$ = \begin{pmatrix}
    \cos(\varphi) & \sin(\varphi)\\
    \sin(\varphi) & \cos(\varphi)
\end{pmatrix}

$U$ $A$-инвариантно $\implies$ $U^\top$  $A^*$ - инвариантно $\implies$ $U^\perp$ инвариантно относительно $A^{-1} \in O(U) \implies$

$A^{-1}: U^\perp \leadsto U^\perp$

$A : U^\perp \leadsto U^\perp $\

$A^{-1} \in Aut(U)$

\textbf{Вывод:} $U^\top A$-инвариантно

$V = U \bigoplus U^\perp, \ dim(U^\perp) <n$ по индукционному предположению в $U^\perp$ существует канонический вид.

\textbf{Единственность}

$\chi_A = \prod^r_{i = 1} ((t^2 - 2t\cos(\varphi_i) + 1)(t-1)^q(t+1)^p$

Все эти корни определенены однозначно с точностью до перестановки блоков. $\square$

\subsection{Комплексификация
 ортогонального оператора — унитарный оператор}
 
Пусть $V \rightsquigarrow (V_\mathbb{C}, ($_$,$_$)_\mathbb{C})$ — эрмитово пространство.

Тогда $A \in O(V) \rightsquigarrow A_\mathbb{C} $ сохраняет эрмитово скалярное произведение
\[
(A_\mathbb{C}(a+bi),A_\mathbb{C}(c+di))_\mathbb{C} = (A(a) + A(b)i, A(c) + A(d)i)_\mathbb{C} = 
\]
\[= (A(a), A(c)) + (A(b), A(d)) + i((A(b), A(c))-(A(a),A(d)) =
\]
\[
= (a,c) + (b,d) + i((b,c) - (a,d)) = (a+bi, c+di)_\mathbb{C}
\]
Получаем, что $A_\mathbb{C}$ — унитарный оператор, $A_\mathbb{C} \in O(V_\mathbb{C)}$.

\subsection{Собственные значения и собственные вектора
 унитарного оператора}
Пусть $A_\mathbb{C} \in O(V_\mathbb{C})$, тогда: 

$(A_\mathbb{C}x,A_\mathbb{C}x) = (\lambda x,\lambda x)_\mathbb{C}= \lambda \overline{\lambda}(x,x)_\mathbb{C}= |\lambda|^2(x,x)_\mathbb{C}= (x,x)_\mathbb{C} \implies |\lambda$|^2 = 1 => |\lambda$| = 1,$

где $\lambda$ это собственное значение, $\lambda \in \mathbb{C}$, а $x$ — собственный вектор.

Собственные значения унитарного оператора лежат на единичной окружности:
$$\lambda = e^{i\varphi} = \cos(\varphi) + i\sin(\varphi)$$

\textbf{Лемма.} Собственные векторы унитарного оператора, соответствующие
 разным собственным значениям, ортогональны.

 \textbf{Доказательство.}
 Следствие из леммы о собственных векторах ортогонального оператора. $\square$
 
 \section{Самосопряжённые и кососимметрические операторы, канонический вид их матриц. Приведение квадратичной функции к главным осям. Собственные векторы самосопряжённого оператора. Комплексификация самосопряжённого оператора эрмитов оператор. Собственные значения и собственные векторы эрмитова оператора.}

 \subsection*{Самосопряжённый оператор, канонический вид его матрицы, собственные век-
торы самосопряжённого оператора. } \newline

\textbf{Опеределение.} Оператор $A$ в евклидовом пространтсве называют самосопряженным, если $A^* = A$; $(x, A(y)) = (A(x), y)$\newline
\textbf{Свойства:}\newline
1) $(A+B)^* = A^*+B^*$\newline
2) $(E)^* = E$\newline
3) $(\lambda A)^* =\lambda  A^*$ для $\lambda \in \mathbb{R}$\newline
4) $(A*B)^* = B^**A^*$\[\]

\textbf{Следствие из теоремы о собственных значениях эрмитова оператора}
Так как при комплесификации самосопряженного оператора, мы получили эртимов оператор, то:\newline
$[A] = [A_\mathbb{C}], X_{A_\mathbb{C}} = X_A => собственные значения A \in \mathbb{R}$\[\]

\textbf{Лемма о собственных векторах}\newline
Собсвтенные векторы самоспряженного оператора, соответсвующие разным собственным значения - ортогональны. \newline
Доказательство: \newline
Выберем ненулевые собственные векторы $x, y \ne 0$ соответсвующие разным собственным значениям $\lambda, \mu \in \mathbb{R}, \lambda \ne \mu$, тогда: \newline
$(x, A(y)) = (A(x), y) => \mu(x,y) = \lambda(x,y) => (\mu - \lambda)(x,y)=0 => (x,y) = 0$\[\]

\textbf{Теорема}\newline
Для любого самосопряженного оператора существует ортонормированный базис, в котором его матрица диагональна и она единственна с точностью до перестановки диагиагональных элементов.\newline
Доказательство: \newline

I) У любого самосопряженного оператора есть собсвтенные векторы. У A существует одномерное или двумерное инвариатное продпространство U\newline
1) $dim(U) = 1 => U = <e_1>, A(e_1)\in <e_1>$\newline
2) $dim(U) = 2 => U = <e_1, e_2>$ они ортогональны\newline

$(A(x), y) = (x, A(y))$ для любого $x,y \in$ V или U\newline
$A|_U = A^*|_U = (A|_U)^* = > [A|_U]_{<e_1, e_2>} =$\begin{bmatrix}
    a&b\\
    b&c
\end{bmatrix}$\in M_2^+(\mathbb{R})$\newline

$X_{A|_U} = t^2 - (a+c)t + (ac-b^2)$\newline
$D = (a+c)^2 - 4(ac-b^2) \geq 0 => $\newline
У $X_{A|_U}$ есть вещественные корни => у $X_A$ есть вещественные корни\newline

II) Пусть $e_1$ - собственные вектор оператора A, пронормируем его, он пораждает $<e_1> = U, dim(U)=1$\newline

Рассмотрим $<e_1>^\perp:$ он инвариантен относительно $A^* = A$ и можно рассмотреть $A|_{<e_1>^\perp}$ - снова самосопряженный оператор, и у него есть вещественные собственные векторы/значения\newline

$e_2 \in <e_1>^\perp$, тоесть $e_2 \perp e_1$ и т.д.\newline

$[A|_{<e_1>}] = (\lambda_2),...,[A|_{<e_i>}] = (\lambda_i)$\newline

A = \begin{bmatrix}
    A_1& 0&...&0\\
    0&A_2 &...&0\\
    0&...&...&0\\
    0&...&0&A_n
\end{bmatrix} = 
\begin{bmatrix}
    \lambda_1& 0&...&0\\
    0&\lambda_2 &...&0\\
    0&...&...&0\\
    0&...&0&\lambda_n
\end{bmatrix} \[\]


 \subsection*{Кососимметрические операторы,
канонический вид их матриц. }

\textbf{Опеределение.} Линейный оператор A называют кососиметрическим, если для любых векторов $x,y \in V$ выполяентся: \newline
$(A(x),y) = -(x,A(x))$\[\]

\textbf{Теорема}\newline
Пусть A - кососиметрический оператор на евклидовом векторном пространстве V. Тогда существует такой ортонормированный базис $<e_1,...,e_n>$, в котором матрица A имеет следующий вид:
A = \begin{bmatrix}
    \begin{bmatrix}
        0&c_1\\
        -c_1& 0
    \end{bmatrix}&0&...&...&...&0\\
    0&...&...&...&...&0\\
    0&...&\begin{bmatrix}
        0&c_r\\
        -c_r& 0
    \end{bmatrix}&...&...&0\\
    0&...&....&0&...&0\\
    0&...&....&...&...&0\\
    0&....&....&...&...&0
\end{bmatrix}\newline
Здесь $c_1,...,c_r > 0$ и $r>=0$\newline
Доказательство:\newline
Докажем индукцией по $dim(V) = n$\newline

\underline{База индукции:} $n=1$ - в ортонормированном базисе $A = (a)$ кососиметрична => $a=0$.\newline
$n=2$ - в ортонормированном базисе $<e_1,e_2>$ матрица оператора A кососиметрична => A = \begin{bmatrix}
    0&c\\
    -c&0
\end{bmatrix}. Если $c \geq 0$, то имеем канонический вид, а если $c <0$, то поменяем базисные векторы местами.\newline
\underline{Шаг индукции:} существует инвариантное относительно A подпространство $U \subseteq V$, $dim(U) =$ 1 или 2, тогда $U^\perp$ инвариантно относительно $A^* = -A$, следовательно, инвариантно относительно A тоже. По предположению индукции, существуют ортонормированные базисы U и $U^\perp$, в которых $A|_U$ и $A|_{U^\perp}$ имеют матрицы $A_1$ и $A_2$ в каноническом виде. Объединяем их, получим ортонормированный базис $V = U \oplus U^\perp$, в котором матрица оператора A имеет матрицу\newline
A = \begin{bmatrix}
    A_1&0\\
    0&A_2
\end{bmatrix}, то есть матрицу канонического вида, с точностью до перестановки блоков\newline
\underline{Единственность:} обозначим A(c) = \begin{bmatrix}
    0&c\\
    -c&0
\end{bmatrix}. Тогда характерестический многочлен оператора A имеет вид:\newline
$X_A(t) =X_{A(c_1)}(t)*...*X_{A(c_r)}(t) = (t^2 +c_1^2)*...*(t^2 + c_r^2)*t^{n-2r}$\newline

Комлексные корни: $\pm ic_1, ..., \pm ic_r, 0$(кратности n-2r). Так как $X_A(t)$ не зависит от выбора базиса, то его корни не зависят от выбора базиса, а значит, и набор $c_1, ...,c_r$ не зависит от выбора базиса.\[\]

\subsection*{Приведение квадратичной функции к главным осям.}

Пусть $V$ -- вещественное евклидово пространство, $Q \colon V \to \mathbb{R}$ -- квадратичная форма с симметричной матрицей $A$. Тогда существует ортонормированный базис $\{e_1, \dots, e_n\}$, в котором $Q$ принимает вид:
\[
Q(x) = \lambda_1 y_1^2 + \lambda_2 y_2^2 + \dots + \lambda_n y_n^2,
\]
где $\lambda_k \in \mathbb{R}$ -- собственные значения оператора $\mathcal{A}$, заданного матрицей $A$, а $y_k = (x, e_k)$.
\end{theorem}

\textbf{Алгоритм}
\begin{enumerate}
  \item \textbf{Матрица формы:} Для $Q(x) = \sum_{i,j} a_{ij}x_i x_j$ построить симметричную матрицу $A = (a_{ij})$.
  \item \textbf{Собственные значения:} Решить $\det(A - \lambda I) = 0$.
  \item \textbf{Собственные векторы:} Для каждого $\lambda_k$ решить $(A - \lambda_k I)\mathbf{v}_k = \mathbf{0}$ и ортонормировать их.
  \item \textbf{Матрица перехода:} Построить $P = \begin{pmatrix} | & & | \\ \mathbf{v}_1 & \cdots & \mathbf{v}_n \\ | & & | \end{pmatrix}$.
  \item \textbf{Канонический вид:} $Q = \sum_{k=1}^n \lambda_k y_k^2$, где $\mathbf{y} = P^{-1}\mathbf{x}$.
\end{enumerate}

\textbf{Доказательство:}\newline
\textbf{1. Симметричный оператор.} 
Пусть $A$ -- матрица $Q$ в ОНБ $\mathcal{E}$. Оператор $\mathcal{A}$ с матрицей $A$ самосопряжён.

\textbf{2. Спектральная теорема.} 
Существует ОНБ $\mathcal{F} = \{e_k\}$ из собственных векторов $\mathcal{A}(e_k) = \lambda_k e_k$.

\textbf{3. Тождество $Q(x) = (\mathcal{A}(x), x)$.} 
В базисе $\mathcal{E}$:
\[
(\mathcal{A}(x), x) = \sum_{i,j} x_i x_j a_{ij} = Q(x).
\]

\textbf{4. Вычисление в $\mathcal{F}$.} 
Для $x = \sum y_k e_k$:
\[
Q(x) = (\mathcal{A}(x), x) = \left( \sum \lambda_k y_k e_k, \sum y_j e_j \right) = \sum \lambda_k y_k^2. \quad \square
\[\]

\textbf{Пример}
Исходная форма: $Q(\mathbf{x}) = 5x_1^2 + 4x_1x_2 + 5x_2^2$.\\
Матрица: $A = \begin{pmatrix} 5 & 2 \\ 2 & 5 \end{pmatrix}$.\\
Собственные значения: $\lambda_{1,2} = 7, 3$.\\
Ортонормированные собственные векторы:
\[
\mathbf{v}_1 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1 \end{pmatrix}, \quad 
\mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1 \end{pmatrix}.
\]
Канонический вид: $Q(\mathbf{y}) = 7y_1^2 + 3y_2^2$.

\subsection*{Комплексификация самосопряжённого оператора эрмитов оператор.}

Пусть:
\begin{itemize}
    \item $V$ — вещественное евклидово пространство со скалярным произведением $(\cdot, \cdot)$,
    \item $\mathcal{A}: V \to V$ — самосопряжённый оператор: $(\mathcal{A}(x), y) = (x, \mathcal{A}(y))$,
    \item $V_\mathbb{C}= V \oplus iV$ — комплексификация $V$ в эрмитово пространство со скалярным произведением:
    \[
    \langle x_1 + iy_1, x_2 + iy_2 \rangle = (x_1, x_2) + (y_1, y_2) + i \left( (y_1, x_2) - (x_1, y_2) \right).
    \]
\end{itemize}

\textbf{Комплексификация оператора:}
\[
\mathcal{A}_\mathbb{C}(x + iy) = \mathcal{A}(x) + i\mathcal{A}(y).
\]

\subsection*{Доказательство эрмитовости}

Для любых $u, v \in V^\CC$:
\begin{align*}
\langle \mathcal{A}_\mathbb{C}(u), v \rangle 
&= \langle \mathcal{A}(x_1) + i\mathcal{A}(y_1), x_2 + iy_2 \rangle \\
&= (\mathcal{A}(x_1), x_2) + (\mathcal{A}(y_1), y_2) + i\left( (\mathcal{A}(y_1), x_2) - (\mathcal{A}(x_1), y_2) \right), \\
\langle u, \mathcal{A}_\mathbb{C}(v) \rangle 
&= \langle x_1 + iy_1, \mathcal{A}(x_2) + i\mathcal{A}(y_2) \rangle \\
&= (x_1, \mathcal{A}(x_2)) + (y_1, \mathcal{A}(y_2)) + i\left( (y_1, \mathcal{A}(x_2)) - (x_1, \mathcal{A}(y_2)) \right).
\end{align*}
Равенство следует из самосопряжённости $\mathcal{A}$ в $V$.

\vspace{-5pt}
\begin{flushright}
    \textit{[Свойство: $\mathcal{A}_\mathbb{C}$ — эрмитов оператор]}
\end{flushright}

\subsection*{Сохранение собственных значений}

\begin{itemize}
    \item Собственные значения $\mathcal{A}$ и $\mathcal{A}_\mathbb{C}$ вещественны
    \item Спектры совпадают: $\sigma(\mathcal{A}) = \sigma(\mathcal{A}_\mathbb{C})$
\end{itemize}

\vspace{-10pt}
\begin{flushright}
    \textit{[Свойство: Сохранение спектра]}
\end{flushright}

\subsection*{Связь собственных векторов}

Если $v \in V$ — собственный вектор $\mathcal{A}$ ($\mathcal{A}(v) = \lambda v$), то $v \in V^\CC$ — собственный вектор $\mathcal{A}_\mathbb{C}$.

Обратно: если $z = x + iy \in V^\CC$ — собственный вектор $\mathcal{A}_\mathbb{C}$, то:
\[
\mathcal{A}(x) + i\mathcal{A}(y) = \lambda (x + iy) \implies \mathcal{A}(x) = \lambda x, \ \mathcal{A}(y) = \lambda y.
\]

\vspace{-10pt}
\begin{flushright}
    \textit{[Свойство: Связь собственных векторов]}
\end{flushright}

 \section{Нормальные операторы в эрмитовом пространстве, их собственные значения и собственные векторы.}

Оператор \( A \) называется \textbf{нормальным}, если:
\[
A A^* = A^* A
\]

\subsection*{Следствия:}

\begin{itemize}
    \item Если \( A \) --- самосопряжённый, то \( A = A^* \Rightarrow AA^* = A^2 = A^*A \Rightarrow \) нормальный.
    \item Если \( A \) --- ортогональный, то \( A^{-1} = A^* \Rightarrow AA^* = A A^{-1} = I = A^{-1}A = A^*A \Rightarrow \) нормальный.
    \item Докозательство:
    \item Если \( A \) --- кососимметричный, то \( A^* = -A \Rightarrow AA^* = -A^2 = A^*A \Rightarrow \) нормальный.
\end{itemize}

\textbf{Лемма:} Если \( A \) --- нормальный, то \( A - \lambda E \) тоже нормальный.
\\
\textbf{Доказательство:}
\begin{align*}
    (A - \lambda E)(A - \lambda E)^* &= (A - \lambda E)(A^* - \overline{\lambda} E) \\
    &= AA^* - \lambda A^* - \overline{\lambda} A + |\lambda|^2 E = A^*A - \overline{\lambda} A - \lambda A^* + |\lambda|^2 E \\
    &= (A - \lambda E)^*(A - \lambda E) \ \ \square
\end{align*}

\textbf{Теорема:} Собственные векторы нормального оператора A соответствующие собственному значению \(\lambda\) являются обственными векторами сопряжённого оператора \(A^*\) соответствующие собственному значению \(\overline{\lambda}\) 
\\
\textbf{Доказательство:}
\begin{multline*}
\text{Пусть } A(x)=\lambda x; \quad B(x)=(A(x)-\lambda \varepsilon x)=0 
\Rightarrow (B(x),B(x))=(0,0)=0 \\
\Rightarrow (x,B^*(B(x)))=0 \Rightarrow (x,B(B^*(x)))=0 \text{ (так как $B$ нормальный)} \\
\Rightarrow (B^*(x),B^*(x))=0 \Rightarrow B^*(x)=0 
\Rightarrow (A^* - \overline{\lambda} \varepsilon)(x)=0 \Rightarrow A^*(x)=\overline{\lambda}x \ \ \square
\end{multline*}


\textbf{Лемма:} Собственные векторы нормального оператора \( A \) с различными собственными значениями --- ортогональны.
\\
\textbf{Доказательство:}
Пусть:
\[
A(x) = \lambda x, \quad A(y) = \mu y, \quad \lambda \ne \mu
\]

Тогда:
\[
(A(x), y) = (\lambda x, y) = \lambda (x, y), \quad (x, A^*(y)) = (x, \overline{\mu} y) = \mu(x, y)
\]
\[
\lambda (x, y)=\mu(x, y)\Rightarrow (\lambda-\mu)(x, y)=0, но \quad \lambda \ne \mu \Rightarrow (x, y)=0 \Rightarrow x \perp y \ \ \square
\]

\textbf{Теорема:} Для нормального оператора существует ортонормированный базис, в котором \( A \) диагонализуется:
\[
A = U D U^*, \quad D = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)
\]

(где \( \lambda_i \) --- собственные значения оператора \( A \)).\\
\textbf{Доказательство:}

Пусть у \( A \) есть собственное значение \( \lambda \in \mathbb{C} \) и собственный вектор \( x \ne 0 \). Тогда:
\[
 A(x) = \lambda x ,  A^*(x) = \overline\lambda x\\ 
\]
$\langle x \rangle$ \( A \)-инвариантно и  \( A^* \)-инвариантно. \\
Рассмотрим 
\[
(x,A^*(y))\textbf{, где }y \in \langle x \rangle^\perp; (A(x),y)=\lambda(x,y)=0\\ \textbf{ т.к. }y \in \langle x \rangle^\perp;\Rightarrow
\]
\[
\Rightarrow A^*(y) \in \langle x \rangle ^\perp \Rightarrow 
\]
\(\Rightarrow \langle x \rangle^\perp- A^*\) инвариантно\\
аналогично \(\langle x \rangle^\perp - A\) инвариантно\\
\[
V =\langle x \rangle \oplus \langle x \rangle^\perp
\]
Если в \(\langle x \rangle^\perp\) осталось только одно собственное значение, A уже диагональна в базисе \(\langle x \rangle\) и \(\langle x \rangle^\perp\).\\
иначе, повтрояя процедуру для \(\langle x \rangle^\perp\) пока собственных значений не останется, получаем:\\
\[
V = \bigoplus_{i=1}^{n} \langle x_i \rangle \ \ \square
\]
\section{Неотрицательно и положительно определённые симметрические операторы, извлечение корней.}
Симметрический оператор - самосопряжённый. Поскольку самосопряжённые операторы соответствуют симметрическим билинейным формам(\(BL^+(V)\)):
\[
A = A^*, \alpha(x, y) = (x, A(y));\textbf{ }q(x)=(x,A(x));
\]

\begin{itemize}
    \item Если форма положительно определена, то соответствующий оператор \( A \) --- положительно определён.
\end{itemize}

\textbf{Пример:} \( AA^* \) самосопряжённый оператор;

\[
q(x) =(x,A(A^*(x))) = (A(x), A(x))=|A(x)|^2\ge0\Rightarrow AA^*- \textbf{неотрицательно определён}
\]

\textbf{Лемма:} \( \forall A = A^* \), A - положительно определён \( \leftrightarrow
\) все \( \lambda > 0 \).\\
\textbf{Доказательство:}\\
положительно определённому A соответствует $q(x) \implies q(x)$ приводится к каноническому виду, где все коэффициенты $>0$ (например, приведением к главным осям) 
\[
\text{Коэффициенты} = \text{собственные значения}
\]
\textbf{Теорема:}\\
\( \forall\) положительно определённого \(A \in End(V)\) \(\exists! B=\sqrt{A},\) \(B^2=A\);\\
\textbf{Доказательство:}\\
A можно привести к виду \(\tilde{A}=diag(\lambda_1, ... \lambda_n )\), где все \( \lambda_i>0\);\\
Пусть: \(\tilde{B}=diag(\sqrt{\lambda_1}, ... \sqrt{\lambda_n} )\);
\[
\tilde{B}^2 = \tilde{A} = C^T A C
\]
\[
A = C \tilde{B}^2 C^T
\]
\[
\text{Так как } A \text{ — положительно определённая, то } C^{-1} = C^T \\
\]
\[
 A = C \tilde{B}^2 C^T = C \tilde{B} C^T C \tilde{B} C^T = B^2
\]

\section{Полярное разложение невырожденного линейного оператора в евклидовом пространстве.}
\textbf{Теорема о полярном разложении:}
\( \forall A\), где \( det(A)\ne0\) \(\exists!\) ортоганальная матрица U и положительно определённая R, что A=UR\\
\textbf{Доказательство:}\\
рассмотрим \(A^*A\); Пусть \(A=UR\Rightarrow A^*=(UR)^*=R^*U^*=RU^{-1}\)\\ по свойствам положительно определённой и ортогональной матрицы.\\
тогда:
\(A^*A=RU^{-1}UR=R^2\)\\
\(R=\sqrt{A^*A} \Rightarrow R\) положительно определена и единственна\(\Rightarrow\)\\
\(\Rightarrow U\) единственна\\
осталось доказать, что \(U\) - ортоганальна.\\
Рассмотрим: \\
\[
|A(x)|=\sqrt{(A(x),A(x))}=\sqrt{(x,A^*(A(x)))}=\sqrt{(x,R^2(x))}=\sqrt{(R(x),R(x))}=|R(x)|
\]
\[
A(x) \textbf{ меняет длинну x так же, как }R(x)
\]
Пусть y=R(x)
Рассмотрим: \\
\[
|U(y)|=|U(R(x))|=|A(x)|=|R(x)|=|y|
\]
\[
U(x)\textbf{ не меняет длинну } x \Rightarrow U \textbf{ ортогональна}
\]
\textbf{Замечания:}\\
1.для произвольного A теорема верна, во всём кроме единственности;\\
2.можно получить левое полярное разложение A=SU;

\section{Тензоры как полилинейные функции. Примеры тензоров малых валентностей. Арифметические операции над тензорами, тензорное умножение. Тензорный базис и размерность пространства тензоров типа $(p; q)$.}
\textbf{Тензоры как полилинейные функции}
Тензор \(\mathbb{T}^q_p(V)\) - полилинейная функция с p векторным и и q ковекторными аргументами.\\
\(T \in \mathbb{T}^q_p(V)\)\\
\[
T:\underbrace{V \times \cdots \times V}_{q \text{ раз}} \times \underbrace{V^* \times \cdots \times V^*}_{p \text{ раз}}\mapsto F
\]
\textbf{Валентность тензора} --- количество векторных и ковекторных аргументов тензора\\
(например, тензор \(\mathbb{T}^q_p(V)\) имеет валентность $(p,q)$)\\
\textbf{Примеры тензоров малых валентностей:}\\
1. (0,0) --- cкаляры \(\mathbb{T}^0_0(V)=F\);\\
2. (1,0) --- ковекторы \(\mathbb{T}^0_1(V)=V^*\);\\
3. (0,1) --- векторы \(\mathbb{T}^0_1(V)=V\);\\
4. (0,2) --- билинейные функции (т.к. имеют 2 векторных аргумента) \(\mathbb{T}^0_2(V)=BL(V)\);\\
5. (1,1) --- \(\mathbb{T}^1_1(V)=V\otimes V^*\);\\
Пусть \(A \in End(V)\) \(T \in \mathbb{T}^1_1(V):T(x,\omega)=\omega(A(x))\)\\
Рассмотрим значение тензора:\\
\(T(e_i,e_j^*)=e_j^*(A(e_i))=a_{ji}\) из матрицы A\\
\[
\mathbb{T}^1_1(V)\leftrightarrow {MN}_+(V)\leftrightarrow \mathrm{End}(V);
\]\[
\mathbb{T}^1_1(V)\textbf{ это } \mathrm{End}(V);
\]
6. (0,n)? - \( \mathrm{det}()\)\\
Пусть \( V=F^n \Rightarrow \mathrm{det}(x_1 ... x_n)\in F \Rightarrow \mathrm{det}() \in \mathbb{T}^0_n(F^n)\)\\
\textbf{Арифметические операции}\\
1. Сумма тензоров:\\
\[
T,S \in  \mathbb{T}^q_p(V)
\]
\[(T+S)(x_1,...x_p,\omega_1,...\omega_q)=T(x_1,...x_p,\omega_1,...\omega_q)+S(x_1,...x_p,\omega_1,...\omega_q)\]\\
2. Умножение на скаляр:
\[(\lambda T)(x_1,...x_p,\omega_1,...\omega_q)=\lambda (T(x_1,...x_p,\omega_1,...\omega_q))\]\\
3. Тензорное произведение:\\
\[
T\in  \mathbb{T}^q_p(V) \textbf{, }S \in  \mathbb{T}^l_k(V)
\]
\[
[T \otimes S] \in \mathbb{T}^{q+l}_{p+k}(V)
\]
\[
[T \otimes S](x_1,...x_{p+k},\omega_1,...\omega_{q+l})=T(x_1,...x_p,\omega_1,...\omega_q)*S(x_{p+1},...x_{p+k},\omega_{q+1},...\omega_{q+l})
\]
\textbf{Теорема:}
\[
\mathrm{dim}(\mathbb{T}^q_p(V))=n^{p+q} \textbf{ при } \mathrm{dim}(V)=n
\]
\textbf{Доказательство:}\\
\[
\mathbb{T}^q_p(V)=\underbrace{V \otimes \cdots \otimes V}_{q \text{ раз}} \otimes \underbrace{V^* \otimes \cdots \otimes V^*}_{p \text{ раз}}
\]
тензоры вида:
\[
e_{i1}^* \otimes ... e_{iq}^* \otimes e_{j1} \otimes ...\otimes e_{jp} \textbf{ --- образуют базис } \mathbb{T}^q_p(V) \ \ \square
\]
\[\textbf{Важно! здесь <<i1>>,<<jp>> и т.п.}\]
\[\textbf{считаются отдельными НЕ СВЯЗАННЫМИ переменными}\]
\[
T \in \mathbb{T}^q_p(V);
\]
\[
T =\sum_{i1, ... iq, j1, ... jp=1}^n (\underbrace{T^{i1, ... iq}_{j1, ... jp}}_\text{скаляр-коэфициент}*(e^{i1}\otimes ...\otimes e^{iq} \otimes e_{j1} \otimes...\otimes e_{jp}));
\]
уточнение: \(e^{i1}=e^*_{i1}\) и т.д.\\
каждая из (p+q) переменных независимо пробегает от 1 до n \Rightarrow \(\mathrm{dim}(\mathbb{T}^q_p(V))=n^{p+q}\)\\
\textbf{Правило Эйнштейна:}
Если пара индексов встречается снизу и сверху, то сумма по ним подразумевается, но не пишется.\\
Например:
\[
\sum^{n}_{i=1}x_ie_i=x^ie_i \ \ \ \ \sum^n_{i=1}e_i^*\omega_i=e^i\omega_i
\]


\section*{18. Компоненты/координаты тензора, их преобразование при замене базиса. Матричная запись тензоров. Эквивалентное определение тензора как «наборов чисел». Тензорное произведение операторов — оператор на тензорном квадрате.}

\textbf{Напомним:}
Любой тензор \( T \in \mathbb{T}^q_p(V)\) представим в виде,\\
\[
T =\sum_{i1, ... iq, j1, ... jp=1}^n (T^{i1, ... iq}_{j1, ... jp}*(e^{i1}\otimes ...\otimes e^{iq} \otimes e_{j1} \otimes...\otimes e_{j1}));
\]
здесь, \( T^{i1, ... iq}_{j1, ... jp}\) - компонента/координата тензора.\\
\( T^{i1, ... iq}_{j1, ... jp} \in F\), где F- поле над которым построенно пространство V\\
Место компоненты тензора в матричном представлении тензора можно найти следуюшим образом:\\
В \( T^{i1,i2,i3}_{j1,j2,j3,j4}\)\\

i1- номер строки внутри подматрицы своего слоя\\
i2- номер столбца внутри подматрицы своего слоя\\
i3- номер 1 слоя по горизонтали\\
j1- номер 1 слоя по вертикали\\
j2- номер 2 слоя по горизонтали\\
j3- номер 2 слоя по вертикали\\
j4- номер 3 слоя по горизонтали\\
\textbf{То есть координаты читаются слева направо и сверху вниз}\\
\textbf{Примеры:}\\
1. \(T\in  \mathbb{T}^1_1(V), dim(V)=2:\)\\
\[
T=
\begin{bmatrix}
\begin{array}{cc}
T^1_1 & T^1_2 \\
T^2_1 & T^2_2
\end{array}
\end{bmatrix}
\]\\
2. \(T\in  \mathbb{T}^0_2(V), dim(V)=2:\)\\
\[
T=
\begin{bmatrix}
\begin{array}{cc}
T_{11} & T_{12} \\
T_{21} & T_{22}
\end{array}
\end{bmatrix}
\]\\
3. \(T\in  \mathbb{T}^0_2(V), dim(V)=3:\)\\
\[
T=
\begin{bmatrix}
\begin{array}{ccc}
T_{11} & T_{12} & T_{13} \\
T_{21} & T_{22} & T_{23} \\
T_{31} & T_{32} & T_{33} \\
\end{array}
\end{bmatrix}
\]\\
4. \(T\in  \mathbb{T}^2_2(V), dim(V)=2:\)\\
\[
T=
\begin{bmatrix}
\begin{array}{c|c}
\begin{array}{cc}
T^{11}_{11} & T^{12}_{11} \\
T^{21}_{11} & T^{22}_{11}
\end{array}
&
\begin{array}{cc}
T^{11}_{12} & T^{12}_{12} \\
T^{21}_{12} & T^{22}_{12}
\end{array}\\
\hline
\begin{array}{cc}
T^{11}_{21} & T^{12}_{21} \\
T^{21}_{21} & T^{22}_{21}
\end{array} & 
\begin{array}{cc}
T^{11}_{22} & T^{12}_{22} \\
T^{21}_{22} & T^{22}_{22}
\end{array}
\end{array}
\end{bmatrix}
\]\\
5. \(T\in  \mathbb{T}^3_2(V), dim(V)=2:\)\\
T=\[
\begin{bmatrix}
\begin{array}{c||c}
\begin{array}{c|c}
\begin{array}{cc}
T^{111}_{11}&T^{121}_{11}\\
T^{211}_{11}&T^{221}_{11}\\
\end{array}
&
\begin{array}{cc}
T^{111}_{21}&T^{121}_{21}\\
T^{211}_{21}&T^{221}_{21}\\
\end{array}\\ 
\hline
\begin{array}{cc}
T^{112}_{11}&T^{122}_{11}\\
T^{212}_{11}&T^{222}_{11}\\
\end{array}
&
\begin{array}{cc}
T^{112}_{21}&T^{122}_{21}\\
T^{212}_{21}&T^{222}_{21}\\
\end{array}
\end{array}
&
\begin{array}{c|c}
\begin{array}{cc}
T^{111}_{12}&T^{121}_{12}\\
T^{211}_{12}&T^{221}_{12}\\
\end{array}
&
\begin{array}{cc}
T^{111}_{22}&T^{121}_{22}\\
T^{211}_{22}&T^{221}_{22}\\
\end{array}\\ 
\hline
\begin{array}{cc}
T^{112}_{12}&T^{122}_{12}\\
T^{212}_{12}&T^{222}_{12}\\
\end{array}
&
\begin{array}{cc}
T^{112}_{22}&T^{122}_{22}\\
T^{212}_{22}&T^{222}_{22}\\
\end{array}
\end{array}
\end{array}
\end{bmatrix}\\
\]
\textbf{Тензорный закон преоброзования координат:}
для начала:
\textbf{Пусть: }
\[ e_{j_k}= \sum^n_{a_k=1} C^{a_k}_{j_k}*\tilde{e}_{a_k}
\]
\[ e^{i_l}= \sum^n_{b_l=1} (C^{-1})^{b_l}_{i_l}*\tilde{e}^{b_l}
\]
здесь,\\
\(e_{j_k}\)- j-тый базисный вектор k-того пространства в разложении T\\
\(C^{a_k}_{j_k}\)- координата матрицы перехода
\((\tilde{e}_{k} \xrightarrow{} e_{k})\) с координатами \(a_k,j_k\)\\
то есть:\\ \(e_{k}=C\tilde{e}_{k}\)\\
\(\tilde{e}_{a_k}\)- a-тый базисный вектор k-того пространства в разложении T в новом базисе\\
\(e^{i_l}\)- i-тый базисный вектор l-того сопряжённого пространства в разложении T\\
\((C^{-1})^{b_l}_{i_l}\) - координата матрицы перехода 
\((\tilde{e}_{k} \xrightarrow{} e_{k})^{-1}\) с координатами \(b_l,i_l\)\\
\textbf{ по свойствам сопряжённого пространства и матрицы перехода если:}
\(e_{k}=C\tilde{e}_{k} \Rightarrow e^{k}=(C^{-1})\tilde{e}^{k}\)\\
\(\tilde{e}^{b_l}\)- b-тый базисный вектор l-того сопряжённого пространства в разложении T в новом базисе\\
так как:
\[
T =\sum_{i1, ... iq, j1, ... jp=1}^n (T^{i1, ... iq}_{j1, ... jp}*(e^{i1}\otimes ...\otimes e^{iq} \otimes e_{j1} \otimes...\otimes e_{jp}))=\]
\[
=\sum_{b1, ... bq, a1, ... ap=1}^n (\tilde{T}^{b1, ... bq}_{a1, ... ap}*(\tilde{e}^{b1}\otimes ...\otimes \tilde{e}^{bq} \otimes \tilde{e}_{a1} \otimes...\otimes \tilde{e}_{ap}))
\]
можем выразить одно из другого.
\[
T^{i1, ... iq}_{j1, ... jp}*(e^{i1}\otimes ...\otimes e^{iq} \otimes e_{j1} \otimes...\otimes e_{jp})=\]
\[
=T^{i1, ... iq}_{j1, ... jp}*
([\sum^n_{b_1=1} (C^{-1})^{b_1}_{i_1}*\tilde{e}^{b_1}]\otimes ...\otimes 
[\sum^n_{b_q=1} (C^{-1})^{b_q}_{i_q}*\tilde{e}^{b_q}] \otimes 
[\sum^n_{a_1=1} C^{a_1}_{j_1}*\tilde{e}_{a_1}]\otimes...\otimes [\sum^n_{a_p=1}C^{a_p}_{j_p}*\tilde{e}_{a_p}])\]
По линейности вынесем суммы:
\[
\sum^n_{b_1,...,bq, a1,...,ap=1}((C^{-1})^{b_1}_{i_1}*...*(C^{-1})^{b_q}_{i_q}*C^{a_1}_{j_1}*...*C^{a_p}_{j_p}*T^{i1, ... iq}_{j1, ... jp}*(\tilde{e}^{b1}\otimes ...\otimes \tilde{e}^{bq} \otimes \tilde{e}_{a1} \otimes...\otimes \tilde{e}_{ap})=
\]
\[
=\tilde{T}^{b1, ... bq}_{a1, ... ap}*(\tilde{e}^{b1}\otimes ...\otimes \tilde{e}^{bq} \otimes \tilde{e}_{a1} \otimes...\otimes \tilde{e}_{ap})
\]
Итого:\\
\[
\sum^n_{b_1,...,bq, a1,...,ap=1}((C^{-1})^{b_1}_{i_1}*...*(C^{-1})^{b_q}_{i_q}*C^{a_1}_{j_1}*...*C^{a_p}_{j_p}*T^{i1, ... iq}_{j1, ... jp}=\tilde{T}^{b1, ... bq}_{a1, ... ap}
\]
\textbf{Альтернативное определение тензора как набора чисел:}\\
Тензор \(T \in \mathbb{T}^q_p(V)\) - набор чисел для каждой компоненты
(\(T^{i1, ...iq}_{j1, ... jp})\)) которого, в каждом базисе при переходе между любыми двумя базисами выполняется тензорный закон преобразования координат.\\
*крч если закон при смене базиса выполняется то такой набор чисел - тензор\\
\textbf{Тензорное произведение операторов:}\\
\textbf{Пусть:}
\[
 A: V\xrightarrow{}W; \textbf{ } e_i \textbf{ - базис }V;\textbf{ } \epsilon_i \textbf{ - базис }W;
\]
\[
 B: \tilde{V}\xrightarrow{}\tilde{W}; \textbf{ } e_i \textbf{ - базис }\tilde{V};\textbf{ } \epsilon_i \textbf{ - базис }\tilde{W};
\]
\[
 A(e_k)= \sum^n_{i=1}a_{ik}*\epsilon_i; B(\tilde{e}_k)= \sum^n_{j=1}b_{jk}*\tilde{\epsilon}_j
\]
Тензорным произведением A и B называют такой \(A\otimes B\), что:
\[ A\otimes B:V \otimes \tilde{V}\xrightarrow{} W \otimes \tilde{W}\]
\[
(A\otimes B)(e_k,\tilde{e}_l)=A(e_k)\otimes B(\tilde{e}_l)=[\sum^n_{i=1}a_{ik}*\epsilon_i] \otimes [\sum^n_{j=1}b_{jl}*\tilde{\epsilon}_j]=
\sum^n_{i,j=1}a_{ik}b_{jl}(\epsilon_i \otimes \tilde{\epsilon}_j)
\]
из этого получаем:
\[
[A \otimes B]=
\begin{bmatrix}
a_{11}*B,a_{12}*B, ...,a_{1n}*B \\
 ...\\
a_{n1}*B,a_{n2}*B,...,a_{nn}*B
\end{bmatrix}\\
\]
\textbf{Следствие:}\\
\[
tr([A \otimes B])=a_{11}*B+...+a_{nn}*B= B(a_{11}+...+a_{nn})=trA*trB
\]
\textbf{Оператор на тензорном квадрате:}\\
Тензорным квадратом векторного пространства V нзывают \(V \otimes V\) \\
\(dim(V \otimes V)=n^2\). он содержит всевозможные комбинации \((u \otimes w)\) где, \(u,w \in V\)\\
Линейный оператор на тензорном квадрате-такой T, что:
\[
T \in End(V \otimes V);
\]
\[
T=A \otimes B \textbf{ где, } A,B \in End(V);
\]
\[
T(u \otimes w)=(A \otimes B)(u \otimes w)=A(u)\otimes B(w)
\]
Так как T - всё ещё линейный оператор для него верно:
\[
(T+S)(u \otimes w)=T(u \otimes w)+S(u \otimes w)
\]
\[(\lambda T)(u \otimes w)=\lambda(T(u \otimes w))\]

\section{Свёртка тензоров. Интерпретация операций линейной алгебры в терминах тензорного умножения и свёртки.}

\section{Тензоры в евклидовом пространстве: подъём и опускание индексов.}

\end{document}
