\documentclass[12pt]{article}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{mathrsfs}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{hyperref}


\titleformat{\section}{\normalfont\Large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{0.5em}{}
\setlist[itemize]{noitemsep, topsep=0pt}

\begin{document}

\begin{center}
    {\LARGE \textbf{Линейная алгебра. Модуль: Евклидовы пространства. Тензоры}}\\
    \vspace{0.3em}
    Башков Иван\\
    \vspace{0.3em}
    2 июня 2025 г.
\end{center}

\section{Билинейные функции и их матрицы. Преобразование матрицы билинейной функции при замене базиса. Ранг и ядра билинейной функции. Связь между билинейной функцией и линейным отображением в сопряжённое пространство.}

\subsection{Билинейные функции и их матрицы}

\textbf{Определение.} \textit{Билинейной функцией (формой)} на векторном пространстве $V$ называется отображение 
\[
\beta: V \times V \to \mathbb{F},
\]
которое линейно по каждому аргументу.

Это означает, что:
\begin{itemize}
    \item $\beta(x_1 + x_2, y) = \beta(x_1, y) + \beta(x_2, y)$,
    \item $\beta(\lambda x, y) = \lambda \beta(x, y)$,
    \item $\beta(x, y_1 + y_2) = \beta(x, y_1) + \beta(x, y_2)$,
    \item $\beta(x, \lambda y) = \lambda \beta(x, y)$,
\end{itemize}
для всех $x, x_1, x_2, y, y_1, y_2 \in V$, $\lambda \in \mathbb{F}$.

\vspace{0.5em}
\textbf{Матрица билинейной формы.} Пусть $e_1, \dots, e_n$ — базис в $V$, тогда билинейная форма $\beta$ задаётся значениями $\beta(e_i, e_j)$, которые образуют матрицу $B = (\beta(e_i, e_j))$.

Если $x = \sum x_i e_i$, $y = \sum y_j e_j$, то:
\[
\beta(x, y) = X^\top B Y, \quad \text{где } X = (x_1, \dots, x_n)^\top,\ Y = (y_1, \dots, y_n).
\]

\subsection{Преобразование матрицы билинейной формы при замене базиса}

Пусть $C$ — матрица перехода от базиса $e$ к новому базису $\tilde{e}$. Тогда матрица билинейной формы в новом базисе $\tilde{B}$ связана с исходной по формуле:
\[
\tilde{B} = C^\top B C.
\]

\textbf{Доказательство:}
Пусть $C=(e \rightsquigarrow \tilde{e})$, где $e$ – старый базис, $\tilde{e}$ – новый базис. Тогда\\
$X = (e \rightsquigarrow \tilde{e}) \tilde{X} = C\tilde{X}$, $Y = (e \rightsquigarrow \tilde{e}) \tilde{Y} = C\tilde{Y}$\\
$\beta(x,y)=(C \tilde{X})^\top B(C\tilde{Y})=\tilde{X}^\top(C^\top BC)\tilde{Y}=\tilde{X}^\top B\tilde{Y} \implies \tilde{B} = C^\top BC, \ C \in GL_n(F) \ \square$

\textbf{Следствие.} Ранг билинейной формы не зависит от выбора базиса, так как $C$ невырожденная.

\subsection{Ранг и ядра билинейной функции}

\textbf{Определение.} \textit{Рангом билинейной формы} $\beta$ называется ранг её матрицы:
\[
\mathrm{rk}\, \beta = \mathrm{rk}\, B.
\]

\textbf{Правое ядро:}
\[
R_\beta =\mathrm{Ker} \beta = \{ y \in V \mid \beta(x, y) = 0 \ \ \forall x \in V \}.
\]

\textbf{Левое ядро:}
\[
L_ \beta = \{ x \in V \mid \beta(x, y) = 0 \ \ \forall y \in V \}.
\]

\textbf{Определение.} Форма называется \textit{невырожденной}, если $\mathrm{Ker} \beta = \{0\}$.

\textbf{Лемма.} $\beta{} \text{ невырождена } \Longleftrightarrow rk\beta=n.$

\subsection{Связь с линейным отображением в сопряжённое пространство}

\textbf{Теорема.} Билинейные формы на $V$ канонически изоморфны линейным отображениям из $V$ в его сопряжённое пространство $V^*$.

\textbf{Доказательство:}

1. Для каждой билинейной формы $\beta \in BL(V)$ определим отображение $\varphi(\beta): V \to V^*$ следующим образом:
     $$
     \varphi(\beta)(v) = \beta(v, -),
     $$
     где $\beta(v, -)$ — линейный функционал на $V$, определенный как:
     $$
     \beta(v, -)(w) = \beta(v, w), \quad \forall w \in V.
     $$

2. Линейность $\varphi$. Для любых $\beta_1, \beta_2 \in BL(V)$ и $\alpha \in F$:
       $$
       \varphi(\beta_1 + \alpha \beta_2)(v) = (\beta_1 + \alpha \beta_2)(v, -) = \beta_1(v, -) + \alpha \beta_2(v, -).
       $$
       Это означает:
       $$
       \varphi(\beta_1 + \alpha \beta_2) = \varphi(\beta_1) + \alpha \varphi(\beta_2).
       $$
       Таким образом, $\varphi$ — линейное отображение.

3. Ядро $\varphi$ состоит из всех билинейных форм $\beta$, для которых $\varphi(\beta) = 0$, то есть:
     $$
     \mathrm{Ker} \varphi = \{\beta \in BL(V) \mid \varphi(\beta) = 0\}.
     $$
     Если $\varphi(\beta) = 0$, то для любого $v \in V$:
     $$
     \beta(v, -) = 0,
     $$
     что означает, что $\beta(v, w) = 0$ для всех $v, w \in V$. Следовательно, $\beta = 0$. Таким образом:
     $$
     \mathrm{Ker} \varphi = \{0\}.
     $$

4. Образ $\varphi$ состоит из всех линейных отображений $V \to V^*$, которые можно получить из билинейных форм. Для любого отображения существует $\beta$, что $\varphi(\beta)$ – это именно такое линейное отображение. Таким образом:
     $$
     \mathrm{Im} \varphi \leq \mathrm{Hom}(V, V^*), \ \dim \mathrm{Im}\varphi=\dim \mathrm{Hom}(V,V^*)=\dim BL(V) \implies \mathrm{Im}\varphi = \mathrm{Hom}(V,V^*)
     $$

5. Поскольку $\mathrm{Ker} \varphi = \{0\}$ и $\mathrm{Im} \, \varphi = \mathrm{Hom}(V, V^*)$, отображение $\varphi$ является \textbf{изоморфизмом} между $BL(V)$ и $\mathrm{Hom}(V, V^*)$. $\ \ \square$



\section{Симметрические и кососимметрические билинейные функции, их матрицы. Ортогональное дополнение к подпространству относительно билинейной функции, его свойства.}

\subsection{Симметрические и кососимметрические билинейные функции, их матрицы}

\textbf{Определение.} Билинейная форма $\beta$ называется:
\begin{itemize}
    \item \textit{симметрической} $(BL^+(V))$, если $\beta(x, y) = \beta(y, x) \ \forall x, y \in V$;
    \item \textit{кососимметрической} $(BL^-(V))$, если $\beta(x, y) = -\beta(y, x) \ \forall x, y \in V$;
    \item \textit{антисимметрической (симплектической)}, если $\beta(x,x)=0 \ \forall x \in V$.
\end{itemize}

\textbf{Лемма.} Если $\beta$ антисимметрическая, то $\beta \in BL^-(V)$.

\textbf{Доказательство.}
$0 = \beta(x + y, x + y)
  = \beta(x, x) + \beta(x, y) + \beta(y, x) + \beta(y, y) = \beta(x, y) + \beta(y, x)
  \implies \beta(x, y) = -\beta(y, x).$ \\
В доказательстве использовалось $\beta(x,x)=0$. Это верно при $char \mathbb{F} \ne 2$: $\beta(x,x)=-\beta(x,x) \implies 2\beta(x,x)=0$.


\textbf{Лемма.} $BL(V) = BL^+(V) \oplus BL^-(V)$.

\textbf{Доказательство:}

\textbf{I.} $BL^+(V) \cap BL^-(V) = \{0\}$

Пусть $\beta \in BL^+(V) \cap BL^-(V)$. Тогда:
\[
\beta(x, y) = \beta(y, x) = -\beta(x, y) \implies 2\beta(x, y) = 0 \implies \beta = 0.
\]

\textbf{II.} Для любой билинейной формы $\beta(x, y)$ можно записать:
\[
\beta(x, y) = \frac{\beta(x, y) + \beta(y, x)}{2} + \frac{\beta(x, y) - \beta(y, x)}{2}.
\]
Здесь:
\[
\frac{\beta(x, y) + \beta(y, x)}{2} \in BL^+(V), \quad \frac{\beta(x, y) - \beta(y, x)}{2} \in BL^-(V). \ \ \square
\]

\textbf{Матрица билинейной формы.}
Пусть $B = (\beta(e_i, e_j))$ — матрица формы в базисе $e_1, \dots, e_n$.
\begin{itemize}
    \item $\beta$ симметрическая $\iff B = B^\top$;
    \item $\beta$ кососимметрическая $\iff B = -B^\top$.
\end{itemize}

\textbf{Следствие.} Ранг кососимметрической формы — чётное число.

\subsection{Ортогональное дополнение относительно билинейной формы}

Пусть $\beta$ – симметрическая/кососимметрическая билинейная форма. Векторы $x$ и $y$ \textbf{ортогональны} относительно $\beta$, если $\beta(x,y)=0$.

\textbf{Определение.} Пусть $U \leq V$. Ортогональным дополнением $U^\perp$ относительно формы $\beta$ называется:
\[
U^\perp = \{ y \in V \mid \beta(x, y) = 0 \ \forall x \in U \}.
\]

\textbf{Лемма.} Если $\beta$ невырожденная, то:
\begin{itemize}
    \textbf{I.} $\dim U^\perp = \dim V - \dim U$;\\
    \textbf{II.} $(U^\perp)^\perp = U$.
\end{itemize}

\textbf{Доказательство:}

\textbf{I.} Пусть $ e_1, \dots, e_k, e_{k+1}, \dots, e_n $ — базис пространства $ V $ и $ e_1, \dots, e_k $ — базис подпространства $ U $.

Рассмотрим ортогональное дополнение $ U^\perp $:
\[
U^\perp = \{ y \in V \mid \beta(e_i, y) = 0 \quad \forall i = 1, \dots, k \}.
\]

Пусть $ y = e_1 y_1 + \dots + e_n y_n $. Тогда система уравнений для $ y \in U^\perp $ имеет вид:
\[
\begin{cases}
b_{11} y_1 + \dots + b_{1n} y_n = 0 \\
b_{21} y_1 + \dots + b_{2n} y_n = 0 \\
\vdots \\
b_{k1} y_1 + \dots + b_{kn} y_n = 0
\end{cases}
\]

Это система линейных алгебраических уравнений (СЛАУ). Ранг матрицы системы равен рангу матрицы коэффициентов:
\[
\mathrm{rk} \begin{pmatrix}
b_{11} & \dots & b_{1n} \\
b_{21} & \dots & b_{2n} \\
\vdots & \ddots & \vdots \\
b_{k1} & \dots & b_{kn}
\end{pmatrix}.
\]

Ранг матрицы коэффициентов, в свою очередь, равен $\dim U$, так как $\beta$ невырождена $\implies$ любая подсистема строк в $B$ линейно независима.

Таким образом, размерность $ U^\perp $ вычисляется как:
\[
\dim U^\perp = n - \mathrm{rk} \begin{pmatrix}
b_{11} & \dots & b_{1n} \\
b_{21} & \dots & b_{2n} \\
\vdots & \ddots & \vdots \\
b_{k1} & \dots & b_{kn}
\end{pmatrix} = n - k = \dim V - \dim U.
\]

\textbf{II.} 1. Проверим включение $ U \subseteq (U^\perp)^\perp $:
   - Возьмём произвольный вектор $ u \in U $. По определению $ U^\perp $, для любого $ y \in U^\perp $ выполнено $ \beta(u, y) = 0 $.
   Следовательно, $ u \in (U^\perp)^\perp $, так как $ u $ ортогонален всем векторам из $ U^\perp $.

2. Вычислим размерность $ (U^\perp)^\perp $:
   по первой части, $ \dim U^\perp = n - k $.
   Аналогично, $ \dim (U^\perp)^\perp = \dim V - \dim U^\perp = n - (n - k) = k $.
   Так как $ \dim U = k $, получаем $ \dim (U^\perp)^\perp = \dim U $.

3. Поскольку $ U \subseteq (U^\perp)^\perp $ и размерности совпадают, имеем $ U = (U^\perp)^\perp. \ \ \square$


\textbf{Определение.} Подпространство $U$ называется \textit{невырожденным}, если ограничение формы $\beta|_U$ — невырожденное.

\textbf{Лемма.} $U$ невырожденное $\iff V = U \oplus U^\perp$.

\textbf{Доказательство.}

$\dim U = k, \ \dim U^\perp \geq n-k$

$\Longrightarrow$

$\dim (U+U^\perp)=\dim U + \dim U^\perp - \dim (U \cap U^\perp)$

$U \cap U^\perp = \set {0}, \dim(U+U^\perp)=\dim U+\dim U^\perp \geq k + (n-k) \geq n = \dim V$

$U+U^\perp \leq V \implies V = U \oplus U^\perp$

\Longleftarrow

$V = U \oplus U^\perp \implies V = U + U^\perp$

$n = \dim (U+U^\perp) = \dim U + \dim U^\perp \implies \dim(U \cap U^\perp)=0 \implies U \cap U^\perp = \set{0} \ \ \square$

\textbf{Определение.} Базис $e_1, \dots, e_n$ называется \textit{ортогональным}, если $\beta(e_i, e_j) = 0$ при $i \ne j$.

\textbf{Теорема.} Для любой симметрической билинейной формы на конечномерном пространстве существует ортогональный базис.

\textbf{Доказательство.} Индукция по $n = \dim V$:
\begin{itemize}
    \item База: $n = 1$ — очев.
    \item Шаг: $n = k$. Пусть $e_k \in V$ так, что $\beta(e_k, e_k) \ne 0$. Тогда $\beta$ невырождена на $U = \langle e_k \rangle$, и по невырожденности $\beta|_U$ имеем разложение $V = U \oplus U^\perp$, при этом $\dim U^\perp=k-1$. По предположению индукции в $U^\perp$ существует ортогональный базис. Добавим к нему $e_k$ и получим базис $V$.$\ \ \square$
\end{itemize}

\section{Квадратичные функции, поляризация. Канонический и нормальный виды симметрической билинейной и квадратичной функций.}

\subsection{Квадратичные функции, поляризация}

\textbf{Определение.} Пусть $\beta \in BL^+(V).$ \textit{Квадратичная форма} – $q(x)=\beta(x,x)$:

$$
q(x) =\sum_{1 \leq i \leq j \leq n}{x_i b_{ij}x_j}
$$

\textbf{Поляризация квадратичной формы} — восстановление билинейной формы:

$$
q(x + y) = \beta(x + y, x + y)
         = \beta(x, x) + 2\beta(x, y) + \beta(y, y)
$$

$$
\beta(x, y) = \frac{q(x + y) - q(x) - q(y)}{2}
$$

\subsection{Канонический и нормальный виды}

Для любой симметрической билинейной формы существует базис, в котором она имеет вид:
$$
\beta(x, y) = \sum_{i=1}^n a_i x_i y_i.
$$

Соответствующая квадратичная форма:
$$
q(x) = \sum_{i=1}^n a_i x_i^2.
$$

Нормальный вид — канонический вид, в котором все ненулевые коэффициенты равны $\pm 1$.

\vspace{2cm} 

\begin{center}
\textbf{Билет №5} \\
\bigskip
\textbf{Положительно/отрицательно определённые билинейные и квадратичные функции. Критерий Сильвестра}
\end{center}

\bigskip

\section*{Билинейные формы}

\textbf{Определение:} Билинейная форма $B(\vec{x}, \vec{y})$ называется \textbf{положительно определённой}, если $\forall \vec{x} \in L_n$, $\vec{x} \neq \vec{0}: B(\vec{x}, \vec{x}) > 0$.

\bigskip

\textbf{Определение:} Билинейная форма $B(\vec{x}, \vec{y})$ называется \textbf{отрицательно определённой}, если $\forall \vec{x} \in L_n$, $\vec{x} \neq \vec{0}: B(\vec{x}, \vec{x}) < 0$.

\bigskip

\section*{Полуторалинейные формы}

\textbf{Определение:} Полуторалинейная форма $B(\vec{x}, \vec{y})$, заданная в $L_n(\mathbb{C})$, называется \textbf{положительно определённой}, если $\forall \vec{x} \in L_n$, $\vec{x} \neq \vec{0}: B(\vec{x}, \vec{x}) > 0$.

\bigskip

\textbf{Определение:} Полуторалинейная форма $B(\vec{x}, \vec{y})$, заданная в $L_n(\mathbb{C})$, называется \textbf{отрицательно определённой}, если $\forall \vec{x} \in L_n$, $\vec{x} \neq \vec{0}: B(\vec{x}, \vec{x}) < 0$.

\bigskip

Отметим, что $B(\vec{x}, \vec{x}) = \overline{B(\vec{x}, \vec{x})}$.

\bigskip

\section*{Квадратичные формы}

\textbf{Определение:} Квадратичная форма $Q(\vec{x})$ называется \textbf{положительно определённой}, если $\forall \vec{x} \neq \vec{0}: Q(\vec{x}) > 0$.

\bigskip

\textbf{Определение:} Квадратичная форма $Q(\vec{x})$ называется \textbf{отрицательно определённой}, если $\forall \vec{x} \neq \vec{0}: Q(\vec{x}) < 0$.

\bigskip

\subsection{Доказательство критерия Сильвестра}

Поскольку в формулировке критерия используется конструкция «тогда и только тогда», доказательство разделяется на две части: \textbf{необходимость} и \textbf{достаточность}.

\subsubsection{Необходимость}
Докажем, что если симметричная билинейная форма $\beta$ положительно определена, то все её угловые миноры положительны.

\textbf{Шаг 1: База индукции ($n = 1$)} \\
При $n = 1$ всё очевидно: матрица билинейной функции $\beta$ в любом базисе представляет собой одну клетку $A = [a_{1,1}]$, а в координатной записи получим
\begin{align*}
q(x) &= a_{1,1} x_1^2 > 0, \\
\Delta_1 &= a_{1,1} > 0.
\end{align*}

Поскольку функция $\beta$ положительно определена на всём пространстве $V$, её ограничение $\beta|_U$ на подпространство $U$ тоже положительно определено. Матрица ограничения $\beta|_U$ (назовём её $A|_U$) получается из матрицы $A$ вычеркиванием последней строки и последнего столбца:

\[
A =
\begin{bmatrix}
a_{1,1} & \cdots & a_{1,k} & a_{1,k+1} \\
\vdots & \ddots & \vdots & \vdots \\
a_{k,1} & \cdots & a_{k,k} & a_{k,k+1} \\
a_{k+1,1} & \cdots & a_{k+1,k} & a_{k+1,k+1}
\end{bmatrix}, \quad
A|_U =
\begin{bmatrix}
a_{1,1} & \cdots & a_{1,k} \\
\vdots & \ddots & \vdots \\
a_{k,1} & \cdots & a_{k,k}
\end{bmatrix}.
\]

Видно, что угловые миноры $\Delta_1, \dots, \Delta_k$ у матриц $A$ и $A|_U$ совпадают. Однако $\dim U = k$, а по предположению индукции для положительно определённой $\beta|_U$ все угловые миноры положительны:
\[
\Delta_1 = a_{1,1} > 0, \quad \dots, \quad \Delta_k = \det(A|_U) > 0.
\]

Осталось доказать, что минор $\Delta_{k+1} = \det A > 0$.

По теореме о диагональной матрице обязательно найдётся базис $\{f_1, \dots, f_{k+1}\}$ такой, что матрица $B$ билинейной функции $\beta$ в этом базисе диагональна:
\[
B =
\begin{bmatrix}
b_{1,1} & \cdots & 0 & 0 \\
\vdots & \ddots & \vdots & \vdots \\
0 & \cdots & b_{k,k} & 0 \\
0 & \cdots & 0 & b_{k+1,k+1}
\end{bmatrix}.
\]

Поскольку функция $\beta$ положительно определена, на диагонали матрицы $B$ стоят только положительные элементы:
\[
b_{i,i} = \beta(f_i, f_i) > 0, \quad 1 \leq i \leq k+1.
\]

Следовательно, определитель диагональной матрицы $B$ тоже положительный:
\[
\det B = b_{1,1} \cdot b_{2,2} \cdot \ldots \cdot b_{k+1,k+1} > 0.
\]

С другой стороны, мы знаем, что матрицы $A$ и $B$ связаны с матрицей перехода $T = T_{e \to f}$ по формуле
\[
B = T^T \cdot A \cdot T.
\]

Следовательно, определители этих матриц тоже связаны:
\begin{align*}
\det B &= \det(T^T \cdot A \cdot T) = \\
&= \det(T^T) \cdot \det A \cdot \det T = \\
&= (\det T)^2 \cdot \Delta_{k+1}.
\end{align*}

Здесь мы воспользовались свойствами определителей:

    
- Определитель произведения равен произведению определителей.
    
- При транспонировании матрицы её определитель не меняется.


Выше мы доказали, что $\det B > 0$. Кроме того, $\det T \neq 0$, поскольку матрица перехода не вырождена. Следовательно, $\Delta_{k+1} > 0$, что и требовалось доказать.


\subsubsection{Достаточность}
Докажем, что если все угловые миноры $\Delta_1, \dots, \Delta_n$ матрицы $A$ билинейной функции $\beta$ в некотором базисе $\{e_1, \dots, e_n\}$ положительны, то функция $\beta$ положительно определена.

\textbf{Шаг 1: Формула Якоби} \\
Согласно формуле Якоби, в пространстве $V$ существует базис $\{f_1, \dots, f_n\}$ такой, что квадратичная функция $q$, ассоциированная с билинейной функцией $\beta$, принимает вид:
\[
q(x) = \frac{\Delta_1}{\Delta_0} x_1^2 + \frac{\Delta_2}{\Delta_1} x_2^2 + \ldots + \frac{\Delta_n}{\Delta_{n-1}} x_n^2,
\]
где $\Delta_0 = 1$.

\textbf{Шаг 2: Анализ коэффициентов} \\
Заметим, что коэффициенты $\frac{\Delta_i}{\Delta_{i-1}} > 0$ для всех $1 \leq i \leq n$. Следовательно, $q(x)$ представляет собой сумму $n$ неотрицательных слагаемых. Поскольку вектор $x \neq 0$, хотя бы одно из этих слагаемых точно будет положительным, поэтому $q(x) > 0$, что и требовалось доказать.

\subsubsection{Относительно отрицательно определённых функций}
Аналогично критерию Сильвестра для положительно определённых функций, доказательство для отрицательно определённых функций проводится с той лишь разницей, что в подходящем базисе $\{f_1, \dots, f_n\}$ все элементы диагональной матрицы $B$ будут отрицательными:
\[
B =
\begin{bmatrix}
-|b_{1,1}| & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & -|b_{n,n}|
\end{bmatrix}, \quad b_{i,i} < 0.
\]

Следовательно, определитель диагональной матрицы $B$ будет положительным, если $n$ чётное, и отрицательным, если $n$ нечётное. Это приводит к чередованию знаков угловых миноров, начиная с $\Delta_1 < 0$:
\[
\Delta_1 < 0, \quad \Delta_2 > 0, \quad \Delta_3 < 0, \quad \dots
\]



\section*{Билет №6}
\subsection*{Евклидовы векторные пространства: определение и примеры. Неравенство Коши-Буняковского, неравенство треугольника, теорема Пифагора. Длина и угол между векторами, теорема косину- сов.}

\subsection*{Определение}
\underline{Определение}: Линейное пространство $L_n$ называется \textbf{евклидовым} ($E_n$), если в нём определена симметричная, положительно определённая БФ $G(\vec{x}, \vec{y})$, называемая \textit{скалярным произведением} $(\vec{x}, \vec{y})$.

\subsection*{Определение}
(ещё одно определение для понятности)Чтобы дать определение евклидова пространства, в качестве основы проще всего использовать понятие \textit{скалярного произведения}. Евклидово векторное пространство определяется как \textbf{конечномерное векторное пространство над полем вещественных чисел}, на парах векторов которого задана вещественнозначная функция $(\cdot, \cdot)$, обладающая следующими тремя свойствами:


    
- \textbf{Линейность}: для любых векторов $\vec{u}, \vec{v}, \vec{w}$ и для любых вещественных чисел $a, b$ справедливы соотношения  
    $$
    (a\vec{u} + b\vec{v}, \vec{w}) = a(\vec{u}, \vec{w}) + b(\vec{v}, \vec{w});
    $$
    
- \textbf{Симметричность}: для любых векторов $\vec{u}, \vec{v}$ верно равенство  
    $$
    (\vec{u}, \vec{v}) = (\vec{v}, \vec{u});
    $$
    
- \textbf{Положительная определённость}: $(\vec{u}, \vec{u}) \geq 0$ для любого $\vec{u}$, причём $(\vec{u}, \vec{u}) = 0 \Rightarrow \vec{u} = \vec{0}$.


\subsection*{Примеры}
\underline{Пример 1}: Аффинное пространство, ассоциированное с векторным пространством $V$ над полем $\mathbb{K}$, — это множество $A$ со свободным транзитивным действием аддитивной группы $V$. Если поле $\mathbb{K}$ не указано явно, подразумевается $\mathbb{K} = \mathbb{R}$.

\underline{Пример 2}: Координатное пространство $\mathbb{R}^n$, состоящее из наборов вещественных чисел $(x_1, x_2, \dots, x_n)$, где скалярное произведение определяется формулой:
$$
(\vec{x}, \vec{y}) = \sum_{i=1}^n x_i y_i = x_1 y_1 + x_2 y_2 + \dots + x_n y_n.
$$

\underline{Пример 3}: Пространство многочленов ограниченной степени $P_n$ — множество всех многочленов с вещественными коэффициентами степени не выше $n$:  
$$
p(t) = a_0 + a_1 t + a_2 t^2 + \dots + a_n t^n.
$$  
Скалярное произведение задаётся формулой:  
$$
(p, q) = \int_a^b p(t)q(t)\,dt,
$$  
где $[a, b]$ — фиксированный интервал ($[0, 1]$ или $[-1, 1]$).  

\textit{Комментарий}: Это евклидово пространство, где векторами являются функции. Проверка аксиом скалярного произведения (симметрия, билинейность, положительная определённость) — стандартное упражнение. Норма многочлена:  
$$
\|p\| = \sqrt{\int_a^b [p(t)]^2\,dt}.
$$


\subsection*{Неравенство Коши-Буняковского}
\underline{Формулировка}:  
Пусть дано линейное пространство $L$ со скалярным произведением $\langle x, y \rangle$. Пусть $\|x\| = \sqrt{\langle x, x \rangle}$ — норма, порождённая скалярным произведением, $\forall x \in L$. Тогда для любых $x, y \in L$ выполняется:
$$
|\langle x, y \rangle| \leq \|x\| \cdot \|y\|.
$$
Равенство достигается тогда и только тогда, когда векторы $x$ и $y$ линейно зависимы (\textit{коллинеарны}), или среди них есть нулевой вектор.

\subsection*{Доказательство (метод квадратичной формы)}
Рассмотрим конструкцию $(x + \lambda y, x + \lambda y)$ для любых $x, y \in E_n$ и $\lambda \in \mathbb{R}$. По свойствам скалярного произведения:
$$
(x + \lambda y, x + \lambda y) \geq 0.
$$
Раскроем скалярное произведение:
$$
(x, x) + \lambda (x, y) + \lambda (y, x) + \lambda^2 (y, y) \geq 0.
$$
Учитывая симметричность скалярного произведения $(x, y) = (y, x)$, получаем:
$$
\lambda^2 (y, y) + 2\lambda (x, y) + (x, x) \geq 0.
$$
Это квадратное выражение относительно $\lambda$ должно быть неотрицательным для всех $\lambda \in \mathbb{R}$. Для этого достаточно, чтобы его дискриминант был не положителен:
$$
D_4 = (2(x, y))^2 - 4(x, x)(y, y) \leq 0.
$$
Упрощаем:
$$
4[(x, y)^2 - (x, x)(y, y)] \leq 0 \quad \Rightarrow \quad (x, y)^2 \leq (x, x)(y, y).
$$
Извлекая корень, получаем требуемое неравенство:
$$
|(x, y)| \leq \sqrt{(x, x)(y, y)}.
$$

\subsection*{Доказательство}
Начнём с известной вам геометрической ситуации. Пусть $\vec{a}$ и $\vec{b}$ — два произвольных вектора на плоскости. Скалярное произведение данных векторов по абсолютной величине не превосходит произведения их длин:
$$
|\vec{a} \cdot \vec{b}| \leq |\vec{a}| |\vec{b}|,
$$
поскольку $\vec{a} \cdot \vec{b} = |\vec{a}| |\vec{b}| \cos \varphi$, где $\varphi$ — угол между векторами.

Теперь запишем это неравенство в координатах. Рассмотрим на плоскости прямоугольную систему координат, пусть $\vec{i}$ и $\vec{j}$ — единичные базисные векторы, направленные вдоль координатных осей. Тогда векторы $\vec{a}$ и $\vec{b}$ можно разложить по базисным векторам:
$$
\vec{a} = a_1 \vec{i} + a_2 \vec{j}, \quad \vec{b} = b_1 \vec{i} + b_2 \vec{j}.
$$
причём длины векторов и скалярное произведение выражаются через координаты векторов — числа $a_1, a_2, b_1, b_2$ — следующим образом:
$$
|\vec{a}| = \sqrt{a_1^2 + a_2^2}, \quad |\vec{b}| = \sqrt{b_1^2 + b_2^2}, \quad \vec{a} \cdot \vec{b} = a_1 b_1 + a_2 b_2.
$$
В результате наше неравенство примет вид
$$
|a_1 b_1 + a_2 b_2| \leq \sqrt{a_1^2 + a_2^2} \cdot \sqrt{b_1^2 + b_2^2},
$$
или
$$
(a_1^2 + a_2^2)(b_1^2 + b_2^2) \geq (a_1 b_1 + a_2 b_2)^2.
$$
Это и есть неравенство Коши — Буняковского — Шварца (КБШ) в простейшем случае $n = 2$. Равенство достигается тогда и только тогда, когда координаты векторов пропорциональны: $a_1/b_1 = a_2/b_2$. Действительно, пропорциональность координат равносильна коллинеарности векторов $\vec{a}$ и $\vec{b}$, то есть условию $\cos \varphi = \pm 1$ или $\vec{a} \cdot \vec{b} = \pm |\vec{a}| |\vec{b}|$.

Если провести аналогичные рассуждения с векторами в трёхмерном пространстве, то получится неравенство КБШ для $n = 3$:
$$
(a_1^2 + a_2^2 + a_3^2)(b_1^2 + b_2^2 + b_3^2) \geq (a_1 b_1 + a_2 b_2 + a_3 b_3)^2.
$$
Равенство достигается при $a_1/b_1 = a_2/b_2 = a_3/b_3$.
В общем случае неравенство Коши — Буняковского — Шварца имеет вид
$$
(a_1^2 + a_2^2 + \dots + a_n^2)(b_1^2 + b_2^2 + \dots + b_n^2) \geq (a_1 b_1 + a_2 b_2 + \dots + a_n b_n)^2
$$
для любых двух наборов действительных чисел $a_1, a_2, \dots, a_n$ и $b_1, b_2, \dots, b_n$, причём равенство достигается тогда и только тогда, когда эти наборы пропорциональны: $\dfrac{a_1}{b_1} = \dfrac{a_2}{b_2} = \dots = \dfrac{a_n}{b_n}$ (в записи пропорциональности мы допускаем ноль в знаменателе, когда ноль присутствует и в соответствующем числите).

Для положительных чисел $x_1, \dots, x_n, y_1, \dots, y_n$ неравенство КБШ можно переписать в следующем виде:
$$
(x_1 + \dots + x_n)(y_1 + \dots + y_n) \geq \left( \sqrt{x_1 y_1} + \dots + \sqrt{x_n y_n} \right)^2.
$$

\subsection*{Неравенство треугольника}
Пусть $(X, \|\cdot\|)$ — нормированное векторное пространство, где $X$ — произвольное множество, а $\|\cdot\|$ — определённая на $X$ норма. Тогда по определению последней справедливо:
$$
\|x + y\| \leq \|x\| + \|y\|, \quad \forall x, y \in X.
$$
\subsection*{Доказательство неравенства треугольника (с использованием свойств скалярного произведения)}
\begin{enumerate}
    \item Вычислим квадрат нормы суммы:
    $$
    \|\vec{u} + \vec{v}\|^2 = \langle \vec{u} + \vec{v}, \vec{u} + \vec{v} \rangle.
    $$
    Раскроем скалярное произведение:
    $$
    \|\vec{u} + \vec{v}\|^2 = \langle \vec{u}, \vec{u} \rangle + 2\langle \vec{u}, \vec{v} \rangle + \langle \vec{v}, \vec{v} \rangle = \|\vec{u}\|^2 + 2\langle \vec{u}, \vec{v} \rangle + \|\vec{v}\|^2.
    $$
    
    \item Применим неравенство Коши-Буняковского:  
    В евклидовом пространстве выполняется $|\langle \vec{u}, \vec{v} \rangle| \leq \|\vec{u}\| \cdot \|\vec{v}\|$. Следовательно:
    $$
    \langle \vec{u}, \vec{v} \rangle \leq |\langle \vec{u}, \vec{v} \rangle| \leq \|\vec{u}\| \cdot \|\vec{v}\|.
    $$
    Подставляем в выражение:
    $$
    \|\vec{u} + \vec{v}\|^2 \leq \|\vec{u}\|^2 + 2\|\vec{u}\| \cdot \|\vec{v}\| + \|\vec{v}\|^2.
    $$
    
    \item Преобразуем в полный квадрат:
    $$
    \|\vec{u} + \vec{v}\|^2 \leq (\|\vec{u}\| + \|\vec{v}\|)^2.
    $$
    
    \item Извлекаем квадратный корень:  
    Так как нормы неотрицательны, получаем:
    $$
    \|\vec{u} + \vec{v}\| \leq \|\vec{u}\| + \|\vec{v}\|.
    $$
\end{enumerate}


\subsection*{Теорема Пифагора в евклидовых пространствах (не уверен что это именно то что нужно)}
Если векторы $\vec{u}$ и $\vec{v}$ ортогональны (т.е. их скалярное произведение равно нулю: $\langle \vec{u}, \vec{v} \rangle = 0$), то выполняется:
$$
\|\vec{u} + \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2,
$$
где $\|x\| = \sqrt{\langle x, x \rangle}$ — евклидова норма.

\subsection*{Доказательство}
\begin{enumerate}
    \item Раскрываем квадрат нормы суммы:
    $$
    \|\vec{u} + \vec{v}\|^2 = \langle \vec{u} + \vec{v}, \vec{u} + \vec{v} \rangle.
    $$
    \item Используем свойства скалярного произведения (билинейность и симметричность):
    $$
    \langle \vec{u} + \vec{v}, \vec{u} + \vec{v} \rangle = \langle \vec{u}, \vec{u} \rangle + \langle \vec{u}, \vec{v} \rangle + \langle \vec{v}, \vec{u} \rangle + \langle \vec{v}, \vec{v} \rangle.
    $$
    \item Подставляем условие ортогональности ($\langle \vec{u}, \vec{v} \rangle = \langle \vec{v}, \vec{u} \rangle = 0$):
    $$
    \|\vec{u} + \vec{v}\|^2 = \langle \vec{u}, \vec{u} \rangle + 0 + 0 + \langle \vec{v}, \vec{v} \rangle = \|\vec{u}\|^2 + \|\vec{v}\|^2.
    $$
\end{enumerate}

\subsection*{Теорема косинусов в евклидовом пространстве}
Для любых векторов $\vec{u}$ и $\vec{v}$ с углом $\theta$ между ними:
$$
\|\vec{u} - \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2 - 2\|\vec{u}\| \cdot \|\vec{v}\| \cos \theta.
$$

\subsection*{Доказательство теоремы косинусов}
\begin{enumerate}
    \item Раскроем квадрат нормы разности векторов через скалярное произведение:
    $$
    \|\vec{u} - \vec{v}\|^2 = \langle \vec{u} - \vec{v}, \vec{u} - \vec{v} \rangle.
    $$
    \item Используем билинейность и симметричность скалярного произведения:
    $$
    \langle \vec{u} - \vec{v}, \vec{u} - \vec{v} \rangle = \langle \vec{u}, \vec{u} \rangle - 2\langle \vec{u}, \vec{v} \rangle + \langle \vec{v}, \vec{v} \rangle = \|\vec{u}\|^2 - 2\langle \vec{u}, \vec{v} \rangle + \|\vec{v}\|^2.
    $$
    \item Подставляем определение угла ($\langle \vec{u}, \vec{v} \rangle = \|\vec{u}\| \cdot \|\vec{v}\| \cos \theta$):
    $$
    \|\vec{u} - \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2 - 2\|\vec{u}\| \cdot \|\vec{v}\| \cos \theta.
    $$
\end{enumerate}


\subsection*{Следствия}
\begin{enumerate}
    \item \textbf{Теорема Пифагора} (при $\theta = 90^\circ$):
    $$
    \|\vec{u} - \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2.
    $$
    \item \textbf{Вычисление угла}:
    $$
    \cos \theta = \frac{\|\vec{u}\|^2 + \|\vec{v}\|^2 - \|\vec{u} - \vec{v}\|^2}{2\|\vec{u}\| \cdot \|\vec{v}\|}.
    $$
    \item \textbf{Критерий ортогональности}:
    $$
    \|\vec{u}\|^2 + \|\vec{v}\|^2 = \|\vec{u} - \vec{v}\|^2 \iff \vec{u} \perp \vec{v}.
    $$
\end{enumerate}

\textit{Теорема верна для всех евклидовых пространств: $\mathbb{R}^n$, пространств матриц, многочленов, функций.}

\section*{Билет №7}
\subsection*{Ортогональность векторов. Ортогональное дополнение к подпространству, его свойства. Ортогональная проекция и ортогональная составляющая. Ортонормированные базисы и ортогональные матрицы.}

\subsection*{Определения}
\underline{Определение}: Элементы $x, y \in E_n$ называются \textbf{ортогональными}, если  
$$
(x, y) = 0.
$$

\underline{Определение}: Базис $\{e_i\}$ в $E_n$ называется \textbf{ортонормированным}, если  
$$
(e_i, e_k) = \delta_{ik}, \quad \text{где } \delta_{ik} = 
\begin{cases} 
1, & i = k, \\ 
0, & i \neq k 
\end{cases}
$$
— символ Кронекера.

\subsection*{Ортогональное дополнение}
\underline{Определение}: Пусть $V$ — векторное пространство над полем $F$ с билинейной формой $B$. Вектор $u$ ортогонален слева вектору $v$, а вектор $v$ ортогонален справа вектору $u$ тогда и только тогда, когда $B(u, v) = 0$. Левое ортогональное дополнение подпространства $W$ — это множество векторов $x \in V$, ортогональных слева каждому вектору $y \in W$:
$$
W^\perp = \{x \in V : B(x, y) = 0 \ \forall y \in W\}.
$$

\subsection*{Свойства ортогонального дополнения}

    
- Ортогональное дополнение является подпространством, то есть замкнуто относительно сложения векторов и умножения на элемент поля.
    
- Если $X \subseteq Y$, то $Y^\perp \subseteq X^\perp$.
    
- Если форма $B$ является невырожденной, а пространство $V$ конечномерно, то  
    $$
    \dim W + \dim W^\perp = \dim V.
    $$
    
- Если же $V$ — конечномерное евклидово пространство и $B$ — скалярное произведение (или же унитарное пространство и эрмитово скалярное произведение соответственно), то для любого подпространства $W \subseteq V$ выполняется разложение в прямую сумму:
    $$
    V = W \oplus W^\perp.
    $$

\subsection*{Ортогональная проекция и ортогональная составляющая}
В евклидовом пространстве $V$ со скалярным произведением $\langle \cdot, \cdot \rangle$ и подпространством $W \subseteq V$:

\begin{enumerate}
    \item \textbf{Ортогональная проекция}:  
    Обозначается $\mathrm{proj}_W(\vec{y})$. Это вектор $\hat{\vec{y}} \in W$, для которого:
    $$
    \vec{y} - \hat{\vec{y}} \perp W \quad (\text{т.е. } \langle \vec{y} - \hat{\vec{y}}, \vec{w} \rangle = 0 \quad \forall \vec{w} \in W).
    $$
    
    \item \textbf{Ортогональная составляющая}:  
    Обозначается $\mathrm{ort}_W(\vec{y})$. Это вектор:
    $$
    \vec{z} = \vec{y} - \hat{\vec{y}}, \quad \text{где } \vec{z} \perp W.
    $$
\end{enumerate}

\textit{Замечание:} Вектор $\vec{y}$ принадлежит всему пространству $V$, а его проекция $\hat{\vec{y}}$ и составляющая $\vec{z}$ разделяют $V$ на подпространство $W$ и его ортогональное дополнение $W^\perp$.

\subsection*{Ключевые свойства}
\begin{enumerate}
    \item \textbf{Единственность разложения}:  
    $$
    \vec{y} = \hat{\vec{y}} + \vec{z}, \quad \hat{\vec{y}} \in W, \ \vec{z} \in W^\perp.
    $$
    
    \item \textbf{Минимальное расстояние}:  
    $\hat{\vec{y}}$ — ближайший к $\vec{y}$ вектор в $W$:  
    $$
    \|\vec{y} - \hat{\vec{y}}\| = \min_{\vec{w} \in W} \|\vec{y} - \vec{w}\|.
    $$
    
    \item \textbf{Теорема Пифагора}:  
    $$
    \|\vec{y}\|^2 = \|\hat{\vec{y}}\|^2 + \|\vec{z}\|^2.
    $$
\end{enumerate}

\subsection*{Формулы для вычисления}
\begin{enumerate}
    \item \textbf{Проекция на вектор $\vec{u}$}:  
    Если $W = \mathrm{span}(\vec{u})$, то  
    $$
    \hat{\vec{y}} = \frac{\langle \vec{y}, \vec{u} \rangle}{\langle \vec{u}, \vec{u} \rangle} \vec{u}, \quad \vec{z} = \vec{y} - \hat{\vec{y}}.
    $$
    
    \item \textbf{Проекция на подпространство с ортонормированным базисом}:  
    Если $\{\vec{u}_1, \dots, \vec{u}_k\}$ — ортонормированный базис $W$, то  
    $$
    \hat{\vec{y}} = \sum_{i=1}^k \langle \vec{y}, \vec{u}_i \rangle \vec{u}_i, \quad \vec{z} = \vec{y} - \hat{\vec{y}}.
    $$
\end{enumerate}

\subsection*{Ортогональный и ортонормированный базисы}
\textbf{Ортогональный базис} — это базис, составленный из попарно ортогональных векторов.  
\textbf{Ортонормированный базис} удовлетворяет ещё и условию единичности нормы всех его элементов. То есть это ортогональный базис, где каждый вектор имеет длину 1.

Последнее удобно записывается при помощи \textit{символа Кронекера}:
$$
\langle e_i, e_j \rangle = \delta_{ij},
$$
где  
$$
\delta_{ij} = 
\begin{cases} 
1, & i = j, \\ 
0, & i \neq j 
\end{cases}
$$
— символ Кронекера.  

Это означает, что скалярное произведение любой пары базисных векторов равно нулю, когда они не совпадают ($i \neq j$), и равно единице при совпадающем индексе ($i = j$).

\subsection*{Ортогональная матрица}
\textbf{Ортогональная матрица} $A$ — квадратная матрица с вещественными элементами, для которой выполняется равенство:  
$$
AA^T = A^T A = E,
$$  
где $A^T$ — транспонированная матрица, $E$ — единичная матрица. Это эквивалентно условию:  
$$
A^{-1} = A^T.
$$  

\textit{Комплексным аналогом ортогональной матрицы} является \textbf{унитарная матрица}.  
Ортогональная матрица с определителем $+1$ называется \textbf{специальной ортогональной}.

\subsection*{Канонический вид ортогональной матрицы}
Любая вещественная ортогональная матрица подобна блочно-диагональной матрице следующего вида:
$$
\begin{pmatrix}
\pm 1 & & & \\
& \ddots & & \\
& & \cos \varphi & \sin \varphi \\
& & -\sin \varphi & \cos \varphi
\end{pmatrix},
$$
где блоки $\pm 1$ соответствуют отражениям, а блоки 
$$
\begin{pmatrix}
\cos \varphi & \sin \varphi \\
-\sin \varphi & \cos \varphi
\end{pmatrix}
$$ 
— поворотам на угол $\varphi$. Это следует из теоремы о приведении ортогональной матрицы к каноническому виду через ортогональную эквивалентность.

\subsection*{Связь с унитарными и нормальными матрицами}
Ортогональная матрица $Q$ является \textbf{унитарной}, так как выполняется условие:  
$$
Q^{-1} = Q^*,
$$  
где $Q^*$ — сопряжённая транспонированная матрица. Это следует из равенства $Q^{-1} = Q^T$ для ортогональных матриц ($Q^T = Q^*$ в вещественном случае).

Кроме того, ортогональная матрица является \textbf{нормальной}, так как:  
$$
QQ^* = Q^*Q.
$$  
Для ортогональной матрицы это равенство принимает вид:  
$$
QQ^T = Q^T Q = E,
$$  
что тривиально выполняется.

\section*{Билет №7}
\subsection*{Метод Грамма-Шмидта ортогонализации системы векторов евклидова пространства, их нормы. Процесс ортогонализации Грамма-Шмидта, его алгоритм. QR-разложение.}

\subsection*{Матрица Грама}
\textbf{Определение}: Пусть в евклидовом пространстве $\mathbb{E}$ задано скалярное произведение $\langle \cdot, \cdot \rangle$. Матрицей Грама системы векторов $\{X_1, \dots, X_m\}$ называется квадратная матрица:
$$
G(X_1, \dots, X_m) = 
\begin{pmatrix}
\langle X_1, X_1 \rangle & \langle X_1, X_2 \rangle & \dots & \langle X_1, X_m \rangle \\
\langle X_2, X_1 \rangle & \langle X_2, X_2 \rangle & \dots & \langle X_2, X_m \rangle \\
\vdots & \vdots & \ddots & \vdots \\
\langle X_m, X_1 \rangle & \langle X_m, X_2 \rangle & \dots & \langle X_m, X_m \rangle
\end{pmatrix}
= [G_{jk}]_{j,k=1}^m,
$$
где $G_{jk} = \langle X_j, X_k \rangle$.

\subsection*{Ключевые свойства}

    
- \textbf{Симметричность}:
        
            
- В вещественных пространствах: $G^\top = G$;
            
- В комплексных: $G = G^*$ (эрмитовость).
        
    
    
- \textbf{Положительная полуопределённость}:
        Для любого вектора $x \in \mathbb{R}^m$:  
        $$
        x^\top G x \geq 0.
        $$  
        Равенство нулю $\Leftrightarrow$ линейная зависимость векторов $\{X_1, \dots, X_m\}$.
    
    
- \textbf{Связь с линейной независимостью}:
        Векторы $X_1, \dots, X_m$ линейно независимы $\Leftrightarrow \det(G) > 0$.  
        Определитель матрицы Грама называется \textit{грамианом}.
    
    
- \textbf{Инвариантность относительно базиса}:
        Значения $\langle X_i, X_j \rangle$ не зависят от выбора базиса пространства $\mathbb{E}$.
    
    
- \textbf{Представление через матрицу}:
        Если $A$ — матрица, столбцы которой содержат координаты векторов $X_i$ в ортонормированном базисе, то:
        $$
        G = A^\top A \quad (\text{в вещественном случае}), \qquad G = A^* A \quad (\text{в комплексном случае}).
        $$
    
    
- \textbf{Геометрическая интерпретация}:
        
            
- Диагональные элементы: $G_{ii} = \langle X_i, X_i \rangle = \|X_i\|^2$ (квадрат длины вектора $X_i$);
            
- Внедиагональные элементы: $G_{ij} = \|X_i\| \cdot \|X_j\| \cdot \cos \theta_{ij}$, где $\theta_{ij}$ — угол между $X_i$ и $X_j$;
            
- Определитель $G$: квадрат объема $m$-мерного параллелепипеда, натянутого на векторы $X_1, \dots, X_m$.
        
\subsection*{Грамиан}
\textbf{Определение}: Определителем Грама (грамианом) системы векторов $\{\vec{e}_1, \vec{e}_2, \dots, \vec{e}_n\}$ в евклидовом пространстве называется определитель матрицы Грама:
$$
\begin{vmatrix}
\langle \vec{e}_1, \vec{e}_1 \rangle & \langle \vec{e}_1, \vec{e}_2 \rangle & \dots & \langle \vec{e}_1, \vec{e}_n \rangle \\
\langle \vec{e}_2, \vec{e}_1 \rangle & \langle \vec{e}_2, \vec{e}_2 \rangle & \dots & \langle \vec{e}_2, \vec{e}_n \rangle \\
\vdots & \vdots & \ddots & \vdots \\
\langle \vec{e}_n, \vec{e}_1 \rangle & \langle \vec{e}_n, \vec{e}_2 \rangle & \dots & \langle \vec{e}_n, \vec{e}_n \rangle
\end{vmatrix},
$$
где $\langle \vec{e}_i, \vec{e}_j \rangle$ — скалярное произведение векторов $\vec{e}_i$ и $\vec{e}_j$.

\textit{Свойство}: Грамиан равен нулю тогда и только тогда, когда система векторов $\{\vec{e}_1, \dots, \vec{e}_n\}$ линейно зависима.

\subsection*{Основные свойства и интерпретации}

    
- \textbf{Критерий линейной независимости}:  
    $$
    \begin{aligned}
    \det(G) > 0 &\iff \text{векторы } \vec{v}_1, \dots, \vec{v}_m \text{ линейно независимы}, \\
    \det(G) = 0 &\iff \text{векторы } \vec{v}_1, \dots, \vec{v}_m \text{ линейно зависимы}.
    \end{aligned}
    $$
    
    
- \textbf{Геометрический смысл (квадрат объёма)}:  
    Для линейно независимых векторов $\vec{v}_1, \dots, \vec{v}_m$ в $\mathbb{R}^n$:  
    $$
    [\mathrm{Vol}_m(\vec{v}_1, \dots, \vec{v}_m)]^2 = \det(G),
    $$  
    где $\mathrm{Vol}_m$ — объём $m$-мерного параллелепипеда, натянутого на векторы ($m \leq n$).
    
    
- \textbf{Связь с координатами}:  
    Если векторы заданы в ортонормированном базисе матрицей $A$ (столбцы — координаты векторов), то при $m = n$:  
    $$
    \det(G) = [\det(A)]^2.
    $$
    
    
- \textbf{Геометрическая интерпретация для малых размерностей}:
    
        
- \textit{Один вектор} ($m = 1$):  
        $$
        G = [\langle \vec{v}_1, \vec{v}_1 \rangle] = [\|\vec{v}_1\|^2] \implies \det(G) = \|\vec{v}_1\|^2.
        $$  
        → Квадрат длины вектора.
        
        
- \textit{Два вектора} ($m = 2$):  
        $$
        G = 
        \begin{pmatrix}
        \langle \vec{v}_1, \vec{v}_1 \rangle & \langle \vec{v}_1, \vec{v}_2 \rangle \\
        \langle \vec{v}_2, \vec{v}_1 \rangle & \langle \vec{v}_2, \vec{v}_2 \rangle
        \end{pmatrix}, \quad
        \det(G) = \|\vec{v}_1\|^2 \cdot \|\vec{v}_2\|^2 - \langle \vec{v}_1, \vec{v}_2 \rangle^2 = (\|\vec{v}_1\| \cdot \|\vec{v}_2\| \cdot \sin \theta)^2.
        $$  
        → Квадрат площади параллелограмма.
        
        
- \textit{Три вектора} ($m = 3$):  
        $$
        \det(G) = (\vec{v}_1 \times \vec{v}_2) \cdot \vec{v}_3 \quad \text{(смешанное произведение)}.
        $$  
        → Квадрат объёма параллелепипеда.

\subsection*{Классический процесс Грамма — Шмидта}

\subsubsection*{Алгоритм}
Пусть имеются линейно независимые векторы $\vec{a}_1, \dots, \vec{a}_n$, и пусть $\mathrm{proj}_{\vec{b}} \vec{a}$ — оператор проекции вектора $\vec{a}$ на вектор $\vec{b}$, определённый как  
$$
\mathrm{proj}_{\vec{b}} \vec{a} = \frac{\langle \vec{a}, \vec{b} \rangle}{\langle \vec{b}, \vec{b} \rangle} \vec{b},
$$  
где $\langle \vec{a}, \vec{b} \rangle$ — скалярное произведение векторов $\vec{a}$ и $\vec{b}$.

Классический процесс Грамма — Шмидта выполняется следующим образом:
\begin{align*}
\vec{b}_1 &= \vec{a}_1 & (1) \\
\vec{b}_2 &= \vec{a}_2 - \mathrm{proj}_{\vec{b}_1} \vec{a}_2 & (2) \\
\vec{b}_3 &= \vec{a}_3 - \mathrm{proj}_{\vec{b}_1} \vec{a}_3 - \mathrm{proj}_{\vec{b}_2} \vec{a}_3 & (3) \\
&\vdots \\
\vec{b}_n &= \vec{a}_n - \mathrm{proj}_{\vec{b}_1} \vec{a}_n - \mathrm{proj}_{\vec{b}_2} \vec{a}_n - \cdots - \mathrm{proj}_{\vec{b}_{n-1}} \vec{a}_n & (n)
\end{align*}

На основе каждого вектора $\vec{b}_j$ ($j = 1, \dots, n$) может быть получен нормированный вектор $\vec{e}_j$ единичной длины, определённый как  
$$
\vec{e}_j = \frac{\vec{b}_j}{\|\vec{b}_j\|}.
$$

\textbf{Результаты процесса Грамма — Шмидта}:  
- $\vec{b}_1, \dots, \vec{b}_n$ — система ортогональных векторов;  
- $\vec{e}_1, \dots, \vec{e}_n$ — система ортонормированных векторов.  

Вычисление $\vec{b}_1, \dots, \vec{b}_n$ называется \textit{ортогонализацией Грамма — Шмидта}, а $\vec{e}_1, \dots, \vec{e}_n$ — \textit{ортонормализацией Грамма — Шмидта}.


здесь геометрическая интерпретация, картинки я вставить не смог, НО на сайте хорошо в картинках показано как и зачем:  
\url{https://ru.ruwiki.ru/wiki/Процесс_Грама_—_Шмидта#Дополнительные_толкования} 


\subsection*{QR-разложение матрицы}
\textbf{QR-разложение} — это представление матрицы $A$ в виде произведения ортогональной матрицы $Q$ и верхнетреугольной матрицы $R$:  
$$
A = QR.
$$

\subsection*{Общий алгоритм через процесс Грамма–Шмидта}
Пусть $A \in \mathbb{R}^{n \times m}$ ($n \geq m$) — матрица с линейно независимыми столбцами $\vec{a}_1, \vec{a}_2, \dots, \vec{a}_m$.  
Алгоритм QR-разложения:
\begin{enumerate}
    \item \textbf{Ортогонализация}: Применить процесс Грамма–Шмидта к столбцам матрицы $A$:  
    $$
    \begin{aligned}
    \vec{u}_1 &= \vec{a}_1, \quad \vec{e}_1 = \frac{\vec{u}_1}{\|\vec{u}_1\|}, \\
    \vec{u}_2 &= \vec{a}_2 - \langle \vec{a}_2, \vec{e}_1 \rangle \vec{e}_1, \quad \vec{e}_2 = \frac{\vec{u}_2}{\|\vec{u}_2\|}, \\
    &\vdots \\
    \vec{u}_k &= \vec{a}_k - \sum_{i=1}^{k-1} \langle \vec{a}_k, \vec{e}_i \rangle \vec{e}_i, \quad \vec{e}_k = \frac{\vec{u}_k}{\|\vec{u}_k\|}.
    \end{aligned}
    $$
    Полученные векторы $\vec{e}_1, \dots, \vec{e}_m$ образуют ортонормированный базис.
    
    \item \textbf{Формирование матрицы $Q$}:  
    Столбцы матрицы $Q \in \mathbb{R}^{n \times m}$ — ортонормированные векторы $\vec{e}_1, \dots, \vec{e}_m$:  
    $$
    Q = \begin{bmatrix} 
    \vec{e}_1 & \vec{e}_2 & \cdots & \vec{e}_m 
    \end{bmatrix}.
    $$
    Матрица $Q$ удовлетворяет условию $Q^\top Q = I_m$, где $I_m$ — единичная матрица размера $m \times m$.
    
    \item \textbf{Вычисление матрицы $R$}:  
    Верхнетреугольная матрица $R \in \mathbb{R}^{m \times m}$ вычисляется как $R = Q^\top A$:  
    $$
    R = 
    \begin{bmatrix}
    \langle \vec{a}_1, \vec{e}_1 \rangle & \langle \vec{a}_2, \vec{e}_1 \rangle & \cdots & \langle \vec{a}_m, \vec{e}_1 \rangle \\
    0 & \langle \vec{a}_2, \vec{e}_2 \rangle & \cdots & \langle \vec{a}_m, \vec{e}_2 \rangle \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \langle \vec{a}_m, \vec{e}_m \rangle
    \end{bmatrix}.
    $$
\end{enumerate}

\subsection*{Свойства QR-разложения}

    
- Матрица $Q$ ортогональна: $Q^\top Q = I_m$.
    
- Матрица $R$ — верхнетреугольная, её диагональные элементы положительны, если $\vec{u}_k \neq 0$.
    
- Если $A$ — квадратная невырожденная матрица, то $Q$ также квадратная и ортогональная ($Q^\top = Q^{-1}$).
    
- QR-разложение существует для любой матрицы $A \in \mathbb{R}^{n \times m}$ ($n \geq m$) с полным рангом.



\end{document}
